{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "pre_tokenizer = tokenizer.backend_tokenizer.pre_tokenizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "split sentence into words and compute the occurance freq of each word\n",
    "return dict(word, freq)\n",
    "\"\"\"\n",
    "def compute_word_freq(corpus, pre_tokenizer):\n",
    "    word2freq = defaultdict(int)\n",
    "\n",
    "    for text in corpus:\n",
    "        words_with_offsets = pre_tokenizer.pre_tokenize_str(text)\n",
    "        new_words = [word for word, offset in words_with_offsets]\n",
    "        for word in new_words:\n",
    "            word2freq[word] += 1\n",
    "    \n",
    "    return word2freq\n",
    "        \n",
    "word2freq = compute_word_freq(corpus, pre_tokenizer)\n",
    "print(word2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "split word into its splits\n",
    "return dict(word, splits)\n",
    "\"\"\"\n",
    "def split_words(words):\n",
    "    word2split = {}\n",
    "    for word in words:\n",
    "        tmp = []\n",
    "        for i in range(len(word)):\n",
    "            if i==0:\n",
    "                tmp.append(word[i])\n",
    "            else:\n",
    "                tmp.append(f'##{word[i]}')\n",
    "        word2split[word] = tmp\n",
    "    return word2split\n",
    "    \n",
    "word2split = split_words(word2freq.keys())\n",
    "print(word2split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "get initial alphabet from directly sperating words\n",
    "return list[str]\n",
    "\"\"\"\n",
    "def get_alphabet_from_words(words):\n",
    "    alphabet = set()\n",
    "    for word in words:\n",
    "        alphabet = alphabet.union([c for c in word])\n",
    "\n",
    "    alphabet = sorted(alphabet) \n",
    "    return alphabet\n",
    "\n",
    "\"\"\" \n",
    "get initial alphabet from using words' splits\n",
    "return list[str]\n",
    "\"\"\"\n",
    "def get_alphabet_from_splits(splits):\n",
    "    alphabet = set()\n",
    "    for split in splits:\n",
    "        alphabet = alphabet.union(split)\n",
    "\n",
    "    alphabet = sorted(alphabet) \n",
    "    return alphabet\n",
    "\n",
    "alphabet = get_alphabet_from_words(word2freq.keys())\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<eot>'] + list(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "compute the freq of each pair in these words\n",
    "return dict[pair, int]\n",
    "\"\"\"\n",
    "def compute_pair_freq(word2split, word2freq):\n",
    "    pair2freq = defaultdict(int)\n",
    "    for word, split in word2split.items():\n",
    "        if len(word) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            freq = word2freq[word]\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                pair2freq[pair] += freq\n",
    "    return pair2freq\n",
    "\n",
    "pair2freq = compute_pair_freq(word2split, word2freq)\n",
    "print(pair2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "find the pair with the biggest frequence\n",
    "return pair, int\n",
    "\"\"\"\n",
    "def find_most_frequent_pair(pair2freq):\n",
    "    assert len(pair2freq) >= 1\n",
    "    max_freq = -1\n",
    "    max_freq_pair = None\n",
    "    for pair, freq in pair2freq.items():\n",
    "        if freq > max_freq:\n",
    "            max_freq = freq\n",
    "            max_freq_pair = pair\n",
    "    return max_freq_pair, max_freq\n",
    "\n",
    "pair, freq = find_most_frequent_pair(pair2freq)\n",
    "\n",
    "pair, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "merge two tokens, '##' is considered\n",
    "return str\n",
    "\"\"\"\n",
    "def merge_pair(pair):\n",
    "    return pair[0] + pair[1][2:]\n",
    "\n",
    "merge_pair(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_rule = {}\n",
    "\"\"\" \n",
    "update the splits of words according to one rule of a specific pair of tokens\n",
    "return dict(word, splits)\n",
    "\"\"\"\n",
    "def update_splits(pair, word2split, new_byte=None):\n",
    "    if new_byte is None:\n",
    "        new_byte = merge_pair(pair)\n",
    "    for word, split in word2split.items():\n",
    "        if len(word) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            i = 0\n",
    "            while i < len(split)-1:\n",
    "                if (split[i], split[i+1]) == pair:\n",
    "                    split = split[:i] + [new_byte] + split[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            word2split[word] = split\n",
    "    return word2split\n",
    "\n",
    "merge_rule[pair] = merge_pair(pair)\n",
    "word2split = update_splits(pair, word2split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence.lower() for sentence in corpus]\n",
    "\n",
    "\n",
    "vocab_size = 50\n",
    "word2freq = compute_word_freq(corpus, pre_tokenizer)\n",
    "alphabet = get_alphabet_from_words(word2freq.keys())\n",
    "word2split = split_words(word2freq.keys())\n",
    "alphabet = get_alphabet_from_splits(word2split.values())\n",
    "vocab = ['<eot>'] + list(alphabet)\n",
    "merge_rule = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(vocab) < vocab_size:\n",
    "    # get the pair freq and the biggest\n",
    "    pair2freq = compute_pair_freq(word2split, word2freq)\n",
    "    pair, freq = find_most_frequent_pair(pair2freq)\n",
    "    # merge rule is kept for faster tokenization\n",
    "    merge_rule[pair] = merge_pair(pair)\n",
    "    vocab.append(merge_pair(pair))\n",
    "    # update splits according to the new pair rule\n",
    "    word2split = update_splits(pair, word2split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, pre_tokenizer, merge_rule):\n",
    "    \"\"\" \n",
    "    just like the loop above, using \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    words_with_offsets = pre_tokenizer.pre_tokenize_str(text)\n",
    "    words = [word for word, offset in words_with_offsets]\n",
    "    word2split = split_words(words)\n",
    "    for pair, new_byte in merge_rule.items():\n",
    "        word2split = update_splits(pair, word2split, new_byte)\n",
    "    tokenized_words = sum([word2split[word] for word in words], [])\n",
    "    return tokenized_words\n",
    "\n",
    "tokenize(\"This is not a token.\", pre_tokenizer, merge_rule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
