{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps_Professional\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "pre_tokenizer = tokenizer.backend_tokenizer.pre_tokenizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "split sentence into words and compute the occurance freq of each word\n",
    "return dict(word, freq)\n",
    "\"\"\"\n",
    "def compute_word_freq(corpus, pre_tokenizer):\n",
    "    word2freq = defaultdict(int)\n",
    "\n",
    "    for text in corpus:\n",
    "        words_with_offsets = pre_tokenizer.pre_tokenize_str(text)\n",
    "        new_words = [word for word, offset in words_with_offsets]\n",
    "        for word in new_words:\n",
    "            word2freq[word] += 1\n",
    "    \n",
    "    return word2freq\n",
    "        \n",
    "word2freq = compute_word_freq(corpus, pre_tokenizer)\n",
    "print(word2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', '##h', '##i', '##s'], 'Ġis': ['Ġ', '##i', '##s'], 'Ġthe': ['Ġ', '##t', '##h', '##e'], 'ĠHugging': ['Ġ', '##H', '##u', '##g', '##g', '##i', '##n', '##g'], 'ĠFace': ['Ġ', '##F', '##a', '##c', '##e'], 'ĠCourse': ['Ġ', '##C', '##o', '##u', '##r', '##s', '##e'], '.': ['.'], 'Ġchapter': ['Ġ', '##c', '##h', '##a', '##p', '##t', '##e', '##r'], 'Ġabout': ['Ġ', '##a', '##b', '##o', '##u', '##t'], 'Ġtokenization': ['Ġ', '##t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'], 'Ġsection': ['Ġ', '##s', '##e', '##c', '##t', '##i', '##o', '##n'], 'Ġshows': ['Ġ', '##s', '##h', '##o', '##w', '##s'], 'Ġseveral': ['Ġ', '##s', '##e', '##v', '##e', '##r', '##a', '##l'], 'Ġtokenizer': ['Ġ', '##t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'Ġalgorithms': ['Ġ', '##a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'], 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'], ',': [','], 'Ġyou': ['Ġ', '##y', '##o', '##u'], 'Ġwill': ['Ġ', '##w', '##i', '##l', '##l'], 'Ġbe': ['Ġ', '##b', '##e'], 'Ġable': ['Ġ', '##a', '##b', '##l', '##e'], 'Ġto': ['Ġ', '##t', '##o'], 'Ġunderstand': ['Ġ', '##u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'], 'Ġhow': ['Ġ', '##h', '##o', '##w'], 'Ġthey': ['Ġ', '##t', '##h', '##e', '##y'], 'Ġare': ['Ġ', '##a', '##r', '##e'], 'Ġtrained': ['Ġ', '##t', '##r', '##a', '##i', '##n', '##e', '##d'], 'Ġand': ['Ġ', '##a', '##n', '##d'], 'Ġgenerate': ['Ġ', '##g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'Ġtokens': ['Ġ', '##t', '##o', '##k', '##e', '##n', '##s']}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "split word into its splits\n",
    "return dict(word, splits)\n",
    "\"\"\"\n",
    "def split_words(words):\n",
    "    word2split = {}\n",
    "    for word in words:\n",
    "        tmp = []\n",
    "        for i in range(len(word)):\n",
    "            if i==0:\n",
    "                tmp.append(word[i])\n",
    "            else:\n",
    "                tmp.append(f'##{word[i]}')\n",
    "        word2split[word] = tmp\n",
    "    return word2split\n",
    "    \n",
    "word2split = split_words(word2freq.keys())\n",
    "print(word2split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "get initial alphabet from directly sperating words\n",
    "return list[str]\n",
    "\"\"\"\n",
    "def get_alphabet_from_words(words):\n",
    "    alphabet = set()\n",
    "    for word in words:\n",
    "        alphabet = alphabet.union([c for c in word])\n",
    "\n",
    "    alphabet = sorted(alphabet) \n",
    "    return alphabet\n",
    "\n",
    "\"\"\" \n",
    "get initial alphabet from using words' splits\n",
    "return list[str]\n",
    "\"\"\"\n",
    "def get_alphabet_from_splits(splits):\n",
    "    alphabet = set()\n",
    "    for split in splits:\n",
    "        alphabet = alphabet.union(split)\n",
    "\n",
    "    alphabet = sorted(alphabet) \n",
    "    return alphabet\n",
    "\n",
    "alphabet = get_alphabet_from_words(word2freq.keys())\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<eot>'] + list(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('T', '##h'): 3, ('##h', '##i'): 3, ('##i', '##s'): 5, ('Ġ', '##i'): 2, ('Ġ', '##t'): 7, ('##t', '##h'): 3, ('##h', '##e'): 2, ('Ġ', '##H'): 1, ('##H', '##u'): 1, ('##u', '##g'): 1, ('##g', '##g'): 1, ('##g', '##i'): 1, ('##i', '##n'): 2, ('##n', '##g'): 1, ('Ġ', '##F'): 1, ('##F', '##a'): 1, ('##a', '##c'): 1, ('##c', '##e'): 1, ('Ġ', '##C'): 1, ('##C', '##o'): 1, ('##o', '##u'): 3, ('##u', '##r'): 1, ('##r', '##s'): 2, ('##s', '##e'): 3, ('Ġ', '##c'): 1, ('##c', '##h'): 1, ('##h', '##a'): 1, ('##a', '##p'): 1, ('##p', '##t'): 1, ('##t', '##e'): 2, ('##e', '##r'): 5, ('Ġ', '##a'): 5, ('##a', '##b'): 2, ('##b', '##o'): 1, ('##u', '##t'): 1, ('##t', '##o'): 4, ('##o', '##k'): 3, ('##k', '##e'): 3, ('##e', '##n'): 4, ('##n', '##i'): 2, ('##i', '##z'): 2, ('##z', '##a'): 1, ('##a', '##t'): 2, ('##t', '##i'): 2, ('##i', '##o'): 2, ('##o', '##n'): 2, ('Ġ', '##s'): 3, ('##e', '##c'): 1, ('##c', '##t'): 1, ('##s', '##h'): 1, ('##h', '##o'): 2, ('##o', '##w'): 2, ('##w', '##s'): 1, ('##e', '##v'): 1, ('##v', '##e'): 1, ('##r', '##a'): 3, ('##a', '##l'): 2, ('##z', '##e'): 1, ('##l', '##g'): 1, ('##g', '##o'): 1, ('##o', '##r'): 1, ('##r', '##i'): 1, ('##i', '##t'): 1, ('##h', '##m'): 1, ('##m', '##s'): 1, ('H', '##o'): 1, ('##o', '##p'): 1, ('##p', '##e'): 1, ('##e', '##f'): 1, ('##f', '##u'): 1, ('##u', '##l'): 1, ('##l', '##l'): 2, ('##l', '##y'): 1, ('Ġ', '##y'): 1, ('##y', '##o'): 1, ('Ġ', '##w'): 1, ('##w', '##i'): 1, ('##i', '##l'): 1, ('Ġ', '##b'): 1, ('##b', '##e'): 1, ('##b', '##l'): 1, ('##l', '##e'): 1, ('Ġ', '##u'): 1, ('##u', '##n'): 1, ('##n', '##d'): 3, ('##d', '##e'): 1, ('##s', '##t'): 1, ('##t', '##a'): 1, ('##a', '##n'): 2, ('Ġ', '##h'): 1, ('##e', '##y'): 1, ('##a', '##r'): 1, ('##r', '##e'): 1, ('##t', '##r'): 1, ('##a', '##i'): 1, ('##n', '##e'): 2, ('##e', '##d'): 1, ('Ġ', '##g'): 1, ('##g', '##e'): 1, ('##n', '##s'): 1})\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "compute the freq of each pair in these words\n",
    "return dict[pair, int]\n",
    "\"\"\"\n",
    "def compute_pair_freq(word2split, word2freq):\n",
    "    pair2freq = defaultdict(int)\n",
    "    for word, split in word2split.items():\n",
    "        if len(word) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            freq = word2freq[word]\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                pair2freq[pair] += freq\n",
    "    return pair2freq\n",
    "\n",
    "pair2freq = compute_pair_freq(word2split, word2freq)\n",
    "print(pair2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Ġ', '##t'), 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "find the pair with the biggest frequence\n",
    "return pair, int\n",
    "\"\"\"\n",
    "def find_most_frequent_pair(pair2freq):\n",
    "    assert len(pair2freq) >= 1\n",
    "    max_freq = -1\n",
    "    max_freq_pair = None\n",
    "    for pair, freq in pair2freq.items():\n",
    "        if freq > max_freq:\n",
    "            max_freq = freq\n",
    "            max_freq_pair = pair\n",
    "    return max_freq_pair, max_freq\n",
    "\n",
    "pair, freq = find_most_frequent_pair(pair2freq)\n",
    "\n",
    "pair, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "merge two tokens, '##' is considered\n",
    "return str\n",
    "\"\"\"\n",
    "def merge_pair(pair):\n",
    "    return pair[0] + pair[1][2:]\n",
    "\n",
    "merge_pair(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_rule = {}\n",
    "\"\"\" \n",
    "update the splits of words according to one rule of a specific pair of tokens\n",
    "return dict(word, splits)\n",
    "\"\"\"\n",
    "def update_splits(pair, word2split, new_byte=None):\n",
    "    if new_byte is None:\n",
    "        new_byte = merge_pair(pair)\n",
    "    for word, split in word2split.items():\n",
    "        if len(word) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            i = 0\n",
    "            while i < len(split)-1:\n",
    "                if (split[i], split[i+1]) == pair:\n",
    "                    split = split[:i] + [new_byte] + split[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            word2split[word] = split\n",
    "    return word2split\n",
    "\n",
    "merge_rule[pair] = merge_pair(pair)\n",
    "word2split = update_splits(pair, word2split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence.lower() for sentence in corpus]\n",
    "\n",
    "\n",
    "vocab_size = 50\n",
    "word2freq = compute_word_freq(corpus, pre_tokenizer)\n",
    "alphabet = get_alphabet_from_words(word2freq.keys())\n",
    "word2split = split_words(word2freq.keys())\n",
    "alphabet = get_alphabet_from_splits(word2split.values())\n",
    "vocab = ['<eot>'] + list(alphabet)\n",
    "merge_rule = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(vocab) < vocab_size:\n",
    "    # get the pair freq and the biggest\n",
    "    pair2freq = compute_pair_freq(word2split, word2freq)\n",
    "    pair, freq = find_most_frequent_pair(pair2freq)\n",
    "    # merge rule is kept for faster tokenization\n",
    "    merge_rule[pair] = merge_pair(pair)\n",
    "    vocab.append(merge_pair(pair))\n",
    "    # update splits according to the new pair rule\n",
    "    word2split = update_splits(pair, word2split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Ġ', '##t'): 'Ġt',\n",
       " ('##i', '##s'): '##is',\n",
       " ('##e', '##r'): '##er',\n",
       " ('Ġ', '##a'): 'Ġa',\n",
       " ('Ġt', '##o'): 'Ġto',\n",
       " ('##e', '##n'): '##en',\n",
       " ('t', '##h'): 'th',\n",
       " ('th', '##is'): 'this',\n",
       " ('##o', '##u'): '##ou',\n",
       " ('##s', '##e'): '##se',\n",
       " ('Ġto', '##k'): 'Ġtok',\n",
       " ('Ġtok', '##en'): 'Ġtoken',\n",
       " ('##n', '##d'): '##nd',\n",
       " ('Ġ', '##is'): 'Ġis',\n",
       " ('Ġt', '##h'): 'Ġth',\n",
       " ('Ġth', '##e'): 'Ġthe',\n",
       " ('Ġ', '##h'): 'Ġh',\n",
       " ('##i', '##n'): '##in',\n",
       " ('Ġ', '##c'): 'Ġc',\n",
       " ('Ġa', '##b'): 'Ġab',\n",
       " ('Ġtoken', '##i'): 'Ġtokeni'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'Ġis', 'Ġ', '##n', '##o', '##t', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, pre_tokenizer, merge_rule):\n",
    "    \"\"\" \n",
    "    just like the loop above, using \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    words_with_offsets = pre_tokenizer.pre_tokenize_str(text)\n",
    "    words = [word for word, offset in words_with_offsets]\n",
    "    word2split = split_words(words)\n",
    "    for pair, new_byte in merge_rule.items():\n",
    "        word2split = update_splits(pair, word2split, new_byte)\n",
    "    tokenized_words = sum([word2split[word] for word in words], [])\n",
    "    return tokenized_words\n",
    "\n",
    "tokenize(\"This is not a token.\", pre_tokenizer, merge_rule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
