{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually implement **scaled dot-product attention** (the core mechanism used in transformers), we can break it down into a few key steps. Here's how we can approach it:\n",
    "\n",
    "Scaled dot-product attention computes the attention scores for a query with respect to a set of keys and uses those scores to combine the values.\n",
    "\n",
    "Given:\n",
    "- **Q** (Query matrix)\n",
    "- **K** (Key matrix)\n",
    "- **V** (Value matrix)\n",
    "\n",
    "The steps are:\n",
    "1. Compute the raw attention scores by taking the dot product of **Q** and **K**.\n",
    "2. Scale the attention scores by dividing by the square root of the dimension of the keys  $\\sqrt{d_k}$ , to prevent very large dot product values from making the softmax output too sharp.\n",
    "3. Apply a softmax function to obtain normalized attention weights. \n",
    "4. Use these attention weights to compute a weighted sum of the **V** (Values) to get the output.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Q $ is the query matrix with shape $(n_q, d_k)$ \n",
    "- $ K $ is the key matrix with shape $(n_k, d_k)$\n",
    "- $ V $ is the value matrix with shape $(n_v, d_v)$\n",
    "- $ d_k $ is the dimensionality of the query and key vectors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.简化版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_V1(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim:int=728) -> None:\n",
    "        super(SelfAttention_V1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, X:torch.tensor):\n",
    "        # x shape: (batch_size, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = X.size()\n",
    "        # QKV shape: (batch_size, seq_len, hidden_dim)\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "        \n",
    "        # (batch_size, seq_len, seq_len)\n",
    "        attention_value = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        attention_weight = torch.softmax(attention_value/math.sqrt(hidden_dim), dim=-1)\n",
    "        print(attention_weight)\n",
    "        \n",
    "        output = torch.matmul(attention_weight, V)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "X = torch.rand((3,3,5))\n",
    "print(X.shape)\n",
    "sa = SelfAttention_V1(5)\n",
    "Y = sa(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.效率优化\n",
    "\n",
    "合并QKV权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_V2(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim:int=728) -> None:\n",
    "        super(SelfAttention_V2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(hidden_dim, hidden_dim*3)\n",
    "        \n",
    "    def forward(self, X:torch.tensor):\n",
    "        # x shape: (batch_size, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = X.size()\n",
    "        # QKV shape: (batch_size, seq_len, hidden_dim)\n",
    "        QKV = self.qkv_proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.hidden_dim, -1)\n",
    "        \n",
    "        # (batch_size, seq_len, seq_len)\n",
    "        attention_value = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        attention_weight = torch.softmax(attention_value/math.sqrt(hidden_dim), dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_weight, V)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "X = torch.rand((3,3,5))\n",
    "print(X.shape)\n",
    "sa = SelfAttention_V2(5)\n",
    "Y = sa(X)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 细节\n",
    "\n",
    "1. dropout位置 \n",
    "2. attention_mask\n",
    "3. output_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_V3(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, drop_out_rate:float=0.1) -> None:\n",
    "        super(SelfAttention_V3, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.proj = nn.Linear(dim, dim * 3)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, X, attention_mask):\n",
    "        # X: batch, seq, dim\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        attention_weight = Q @ K.transpose(-1,-2) / math.sqrt(self.dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(attention_mask==0, float('-1e20'))\n",
    "        \n",
    "        attention_weight = torch.softmax(attention_weight, dim=-1)\n",
    "        print(attention_weight)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "        \n",
    "        output = attention_weight @ V\n",
    "        \n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "        \n",
    "X = torch.rand((2,3,5))\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,1,0]\n",
    "    ]\n",
    ")\n",
    "mask = mask.unsqueeze(dim=1).repeat(1,3,1)\n",
    "print(X.shape)\n",
    "print(mask.shape)\n",
    "sa = SelfAttention_V3(5)\n",
    "Y = sa(X, mask)\n",
    "print(Y.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,1,0]\n",
    "    ]\n",
    ")\n",
    "mask = mask.unsqueeze(dim=1).repeat(1,3,1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.面试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_Final(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim:int, drop_out_rate:float=0.1) -> None:\n",
    "        super(SelfAttention_Final, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "        self.key_proj = nn.Linear(dim, dim)\n",
    "        self.value_proj = nn.Linear(dim, dim)\n",
    "        self.attention_dropout = nn.Dropout(drop_out_rate)\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, X:torch.tensor, attention_mask:torch.tensor):\n",
    "    \n",
    "        assert X.shape[:-1] == attention_mask.shape\n",
    "        \n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "        \n",
    "        attention_score = Q @ K.transpose(-1,-2) / math.sqrt(self.dim)\n",
    "        \n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).repeat(1,X.shape[1],1)\n",
    "            attention_score = torch.masked_fill(attention_score, attention_mask==0, -float('inf'))\n",
    "        \n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "        \n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        \n",
    "        output = attention_weight @ V\n",
    "        output = self.output_proj(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "X = torch.rand((2,3,5))\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,1,0]\n",
    "    ]\n",
    ")\n",
    "model = SelfAttention_Final(5, 0.1)\n",
    "y = model(X, mask)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码还有个问题，self attention 和 cross attention其实没有本质的区别，因此最好还是把qkv的输入区分开."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually implement **Multi-Head Attention** in PyTorch, we need to combine several scaled dot-product attention mechanisms (one per head). Each attention head has its own set of query, key, and value weights, and after computing the attention for each head, we concatenate the results and apply a final linear transformation.\n",
    "\n",
    "### Multi-Head Attention Overview:\n",
    "\n",
    "1. **Input**: Given:\n",
    "   - $ X $ (Input matrix) of shape $ (n_{batch}, n_{seq}, d_{embedding}) $\n",
    "   - **$ d_{embedding} $** is the model's dimension (e.g., 512 or 1024)\n",
    "   - **$ n_{\\text{heads}} $** is the number of attention heads (e.g., 8)\n",
    "\n",
    "2. **Linear Projections**: For each attention head, we project $ Q $, $ K $, and $ V $ into smaller dimensions using learned weight matrices:\n",
    "   - $ Q_i = XW_q^i $\n",
    "   - $ K_i = XW_k^i $\n",
    "   - $ V_i = XW_v^i $\n",
    "   where $ W_q^i$ , $ W_k^i $, and $ W_v^i $ are the learned projection matrices for each head $ i $.\n",
    "\n",
    "3. **Scaled Dot-Product Attention**: For each head, we compute the scaled dot-product attention:\n",
    "   $$\n",
    "   \\text{Output}_i = \\text{Attention}(Q_i, K_i) \\cdot V_i = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_i\n",
    "   $$\n",
    "   where $ d_k = \\frac{d_{embedding}}{n_{\\text{heads}}} $.\n",
    "\n",
    "4. **Concatenate Heads**: After computing the attention for each head, we concatenate the results across all heads and apply a final linear transformation to the concatenated vector of shape $ (n_{seq}, d_{embedding}) $.\n",
    "\n",
    "   $$\n",
    "   \\text{MultiHeadOutput} = Concat(\\text{Output}_1, \\text{Output}_2, \\dots, \\text{Output}_H) \\cdot W_{linear}\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim:int, num_head:int, attention_dropout_rate:float=0.1) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert dim % num_head == 0 \n",
    "        self.dim = dim\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim // num_head\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.o_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(attention_dropout_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, query, key, value, attention_mask):\n",
    "        \n",
    "        # assert query.shape == key.shape == value.shape\n",
    "        bs, sl, hs = query.shape\n",
    "        \n",
    "        Q = self.q_proj(query)\n",
    "        K = self.k_proj(key)\n",
    "        V = self.v_proj(value)\n",
    "        print(Q.shape)\n",
    "        \n",
    "        # (b, s, d) => (b, n, s, d/n)\n",
    "        Q_heads = torch.reshape(Q, (bs, sl, self.num_head, self.dim_head)).transpose(1,2)\n",
    "        K_heads = torch.reshape(K, (bs, sl, self.num_head, self.dim_head)).transpose(1,2)\n",
    "        V_heads = torch.reshape(V, (bs, sl, self.num_head, self.dim_head)).transpose(1,2)\n",
    "        print(Q_heads.shape)\n",
    "        \n",
    "        \n",
    "        # (b, n, s, s)\n",
    "        attention_score = Q_heads @ K_heads.transpose(-1,-2) / math.sqrt(self.num_head)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_score = attention_score.masked_fill(attention_mask==0, float('-inf'))\n",
    "            \n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        \n",
    "        # (b, n, s, d/n)\n",
    "        output = attention_weight @ V_heads\n",
    "         # (b, s, n, d/n)\n",
    "        output = output.transpose(1,2)\n",
    "        output = torch.reshape(output, (bs,sl,-1))\n",
    "        \n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "X = torch.rand((2,3,6))\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,1,0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "mask = mask.unsqueeze(1).repeat(1,3,1)\n",
    "model = MultiHeadAttention(6,2, 0.1)\n",
    "y = model(X, X, X, mask)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
