## 手撕Transformer

文档：

1. Transformer模型架构介绍
2. 掩码(Masked)层

代码：

- 注意力Attention原理和实现
- token 和 position Embedding
- LayerNorm
- 残差连接FFN

- Encoder-Decoder实现

参考：

1. 序列模型之王 - Transfomer 全细节详解：https://blog.csdn.net/Kuo_Jun_Lin/article/details/114241287
2. 基于简单数据的英文文本翻译，代码： https://github.com/dt-3t/Transformer-en-to-cn.git，讲解： https://blog.csdn.net/qq_36396406/article/details/132384993
3. 基于transformers的自然语言处理(NLP)入门，代码： https://github.com/datawhalechina/learn-nlp-with-transformers.git

## 手撕BERT





## 手撕GPT

BPE（Byte-Pair Encoding）分词算法



参考：

nanoGPT，代码：https://github.com/karpathy/nanoGPT.git





## 手撕LLAMA



https://blog.csdn.net/qq_35054222/article/details/139123494

llama3 implemented from scratch，代码：https://github.com/naklecha/llama3-from-scratch





## 手撕Alpaca



计算全量微调占显存





Stanford Alpaca: An Instruction-following LLaMA Model，代码： https://github.com/tatsu-lab/stanford_alpaca，运行教程：https://zhuanlan.zhihu.com/p/618321077







## 手撕LORA

LoRA/QLoRA论文解析







### LLM推理优化技术



VLLM



https://github.com/liguodongiot/llm-action/



