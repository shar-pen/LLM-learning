## QLoRA微调

## 模型量化

**模型量化（quantization）**也被叫做模型的低精度表示，指的是在不大幅降低模型效果的前提下使用更低的精度来表示模型中的参数，从而缩减模型的体积和训练模型时占用的显存。量化的本质是函数映射，根据量化过程是否线性我们可以把量化分为**线性量化**和**非线性量化**。量化过程是从一种数据类型“舍入”到另一种数据类型。举个例子，如果一种数据类型的范围为 `0..9`，而另一种数据类型的范围为 `0..4`，则第一种数据类型中的值 `4` 将舍入为第二种数据类型中的 `2` 。但是，如果在第一种数据类型中有值 `3`，它介于第二种数据类型的 `1` 和 `2` 之间，那么我们通常会四舍五入为 `2`。也就是说，第一种数据类型的值 `4` 和 `3` 在第二种数据类型中具有相同的值 `2`。这充分表明量化是一个有噪过程，会导致信息丢失，是一种有损压缩。最常见的**量化技术**是**最大绝对值 (absolute maximum quantization，absmax) 量化**，如式(1)。

$$
\mathbf{X}^{\mathrm{Int} 8}=\operatorname{round}\left(\frac{127}{\operatorname{absmax}\left(\mathbf{X}^{\mathrm{FP} 32}\right)} \mathbf{X}^{\mathrm{FP} 32}\right)=\operatorname{round}\left(c^{\mathrm{FP} 32} \cdot \mathbf{X}^{\mathrm{FP} 32}\right)  \tag1
$$

- $\mathbf{X}^{\mathrm{FP} 32}$：这是原始的 32 位浮点数表示的张量或数据。
- $absmax(\mathbf{X}^{\mathrm{FP} 32})$：这是浮点数张量中绝对值最大的元素。它用来衡量数据的动态范围（即数据中最大值和最小值的尺度）。`absmax` 是一种常用的缩放方法，通过取最大绝对值来估算数据的幅度。
- $127$：这是常见的量化缩放因子之一。在 8 位整数（`Int8`）表示中，整数的取值范围通常是 -128 到 127（即 8 位有符号整数的范围），因此缩放因子使用 127 来将浮动的值映射到这个范围。

举一个例子：

例如，假设你要用 absmax 对向量 `[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]` 进行量化。首先需要计算该向量元素的最大绝对值，在本例中为 `5.4`。 Int8 的范围为 `[-127, 127]`，因此我们将 `127` 除以 `5.4`，得到缩放因子 `23.5`。最后，将原始向量乘以缩放因子得到最终的量化向量 `[28, -12, -101, 28, -73, 19, 56, 127]`。
  
![img](./assets/量化例子.png)
  
  
还有一种**量化技术**叫做**零点量化（zero-point quantization）**，零点量化分为两步，**第一步值域映射**，即通过缩放将原始的数值范围映射为量化后的数值范围; **第二步零点调整**，即通过平移将映射后的数据的最小值对齐为目标值域的最小值。

量化的好处有很多，首先量化可以减小模型的大小，例如在资源有限的手机端经常使用量化后的模型。其次是量化后的模型拥有更快的速度，这在并发量比较高或者对速度要求比较高的场景非常适用。最后是因为一些硬件只支持低精度的运算单位，所以我们需求将模型也转换到相同的精度。因为量化操作不可避免的带来一些误差，例如从float32到int8的round操作带来的精度损失，超出int8范围的溢出值的截断等等。**模型量化的核心工作就是在尽量保证模型准确率的前提下优化模型的推理速度和模型体积**。

与量化对应的是反量化（dequantization），反量化指的是将模型的低精度恢复为高精度的过程，主要用于减少量化造成的精度损失。 

$$
\operatorname{dequant}\left(c^{\mathrm{FP} 32}, \mathbf{X}^{\mathrm{Int} 8}\right)=\frac{\mathbf{X}^{\mathrm{Int} 8}}{c^{\mathrm{FP} 32}}=\mathbf{X}^{\mathrm{FP} 32}  \tag2
$$

按照量化过程是否以0点为对称点量化又可以分为**对称量化**和**非对称量化**。其中对称量化将原浮点数的最小或最大值的绝对值作为映射值的范围，而非对称量化是将原浮点数的最小和最大值映射为量化数据的最小和最大值。在非对称量化中，0的映射也可能会有偏移，因此不一定会被映射到0。
![图1：对称量化和非对称量化](./assets/对称量化和非对称量化.png)
> 图1：对称量化和非对称量化
                                                             

###  分位数量化

分位数量化（Quantile Quantization）是隶属于非线性量化。**分位数**（Quantile）在数学上的定义指的是把**顺序排列**的一组数据分割为若干个相等块的分割点的数值。在标准正态分布中，对于分布$X$给定的概率$\alpha$，如果存在$u_\alpha $使得它的累积分布函数（CDF）$P(X < u_\alpha ) = \alpha$  则称$u_\alpha$是标准正态分布的$\alpha$分位数，如图2。因为CDF在图3中表示的是概率值小于$u_\alpha$的阴影部分的面积，因此具有严格递增的特性，所以它一定存在反函数。CDF的反函数的一个重要作用是用来生成服从该随机分布的随机变量。假设$\alpha$是 [0,1) 区间上均匀分布的一个随机变量，那么$ F_X^{-1}(a) $服从分布 $X$。
![图2：$\alpha$分位数在标准正态分布中的含义](./assets/正态分布.png)
>图2：$\alpha$分位数在标准正态分布中的含义

Tim Dettmers大佬认为，具有k-比特的有损最小熵编码具有如下特性：**当将输入数据进行量化时，每个可能的k-bit的整数值出现的频率是相等的**。这个很好理解，比如我们粗暴的使用round操作去低精度的更近的值，我们可能造成大量的数据都被量化到同一个数上，这样特征之间的差异性在量化过程中就被丢失了。

为了满足这个特性，**我们可以使用分位数将张量分成了大小相同的若干个块**，这样我们得到更加均匀的量化特征，这也就是分位数量化。如图3所示，对于4比特量化，我们希望需要找到15个分位数来将这个曲线下面的面积（积分）等分成16份。两个分位数的中点便是模型量化到这个区间映射的值$q_i$ 。
![](./assets/正态可视化.png)
> 图3：$q_i$在标准正态分布曲线上的可视化
                                                
如果我们通过图3中区域的面积来确定分位数显然很困难。但是我们知道，预训练模型的参数往往是符合正态分布的，因此我们可以通过累积分布函数的反函数$Q_X = F^{-1}_X $来简化分位数的计算。假设我们要将一个张量分成15个块的分位点，我们可以把CDF曲线按照它的$y $等距离分割成16份，然后每个分割点对应的$x $轴上的值便是这个分位点，两个分位点的中心点便是$q_i$，如图4所示。
![](./assets/正态反函数.png)
> 图4：$q_i$在标准正态分布的累积分布函数的可视化

$q_i$的计算可以简化为式(3)。但是我们知道0和1的CDF的反函数的解分别是负无穷和正无穷，因此我们不能将0和1代入式(3)。为了解决这个问题，我们可以设置一个偏移（offset）位。使用偏移位后我们计算的区间不再是 [0,1] ，而是

$$
 [1-\text{offset},\text{offset}] 。 q_i = \frac12\left(Q_X\left( \frac{i}{2^{k}+1} \right) + Q_X\left( \frac{i+1}{2^{k} + 1 } \right)\right) \tag3
$$

有了$q_i$ ，我们便可以进行量化计算了，它的计算过程一般分为三步：

1. 计算归一化常数$ N = \max(|T|)$ ，将输入张量$T$转化到目标量化数据类型的范围内；
2. 对于$T/N$的每个元素，使用二进制搜索在域中找到阈值最接近的对应值$q_i$，其中$Q^\text{map} $是 $q_i$的集合；  

$$
T_i^Q = \mathop{\arg\min}^{2n}_{j=0} \left|Q_j^\text{map} - \frac{T_i}{N}\right| \tag4
$$
3. 将对应$q_i$的索引$i$存储在量化输出的张量中。

### 分块k位量化

在式(1)中，$ \frac{127}{\text{absmax}(\mathbf X^\text{FP32})} $的作用是依据参数中的最大值确定缩放尺度。但是如果这个值是一个异常的极大或者极小值，那么使用它计算缩放尺度就不合适了，因为它会造成整个张量的绝大多数值在量化后都在0附近，从而破坏了量化后特征分布的均匀性。**分块k位量化（block-wise k-bit quantization）**[[3]](https://zhuanlan.zhihu.com/p/666234324#ref_3)的策略是通过将张量分成若干个块，让每个块都有独立的量化常数c，从而解决模型参数的极大极小的异常值的问题。分块量化的另外一个好处是减少了核之间的通信，可以实现更好的并行性，并充分利用硬件的多核的能力。

图5展示了分块k位量化的量化和反量化的一个例子。在量化过程中，状态张量被分块处理，通过每个块的最大值进行归一化，最后得到的是最近的值以及它所在块的最大值。在反量化时，我们根据存储的量化后的低精度的值以及它所在块的最大值恢复到高精度的值。
![](./assets/分块量化.png)
> 图5：分块k位量化的量化和反量化的过程

## QLoRA介绍

QLoRA的工作有三个，

1. 第一个工作是结合了**分位数量化**和**分块量化**的**4位标准浮点数量化**（4-bit NormalFloat Quantization）。
2. 第二个工作是对模型进行两次量化的**双重量化**（Double Quantization），它的第二次量化只作用在第一次量化产生的量化常数上，可以进一步节约显存占用。
3. 第三个工作是**分页优化**（Paged Optimizer），使用CPU内存代替GPU显存保存部分梯度参数。下面我们来详细介绍它们。 
  
![](./assets/qlora技术图.png)  
这个图很好的展示了全量微调，LoRA和QLoRA的演变。

**全量微调中**，16bit的Transformer模型直接进行微调。

**LoRA中**，我们使用了两个低秩矩阵来代替原始的矩阵，降低了模型实际的参数，但是仍要保存16bit的Transformer模型参数。

**QLoRA中**，将16bit的Transformer量化成4bit保存，又进一步的降低了显存，同时又使用了分页优化。

