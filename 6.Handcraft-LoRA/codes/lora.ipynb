{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen2-1.5b推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京奥运会是2008年夏季奥林匹克运动会，这是中国举办的第一次奥运会。该届奥运会于2008年7月26日开幕，8月13日闭幕。\n",
      "\n",
      "在本次奥运会上，中国代表团取得了51枚金牌、21枚银牌和28枚铜牌的优异成绩，位居奖牌榜首位。此外，中国运动员还创下了多项世界纪录和奥运记录。\n",
      "\n",
      "北京奥运会的成功举办不仅展示了中国的经济实力和发展水平，也体现了中国政府对体育事业的支持和投入。\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = '/data01/tqbian/modelPATH/Qwen/Qwen2___5-1___5B-Instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "prompt = \"北京奥运会\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('json', data_files=\"/data01/tqbian/src/learning/LLM_Toturials/LoRa_Qwen2.5/alapca_data_zh/alpaca_gpt4_data_zh.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 48818\n",
      "    })\n",
      "})\n",
      "{'train': ['instruction', 'input', 'output']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': '描述原子的结构。',\n",
       " 'input': '',\n",
       " 'output': '原子是物质的基本单位，它由三种基本粒子组成：质子、中子和电子。质子和中子形成原子核，位于原子中心，核外的电子围绕着原子核运动。\\n\\n原子结构具有层次性。原子核中，质子带正电，中子不带电（中性）。原子核非常小且致密，占据了原子总质量的绝大部分。电子带负电，通常围绕核运动，形成若干层次，称为壳层或电子层。电子数量与质子数量相等，使原子呈电中性。\\n\\n电子在每个壳层中都呈规律分布，并且不同壳层所能容纳的电子数也不同。在最里面的壳层一般只能容纳2个电子，其次一层最多可容纳8个电子，再往外的壳层可容纳的电子数逐层递增。\\n\\n原子核主要受到两种相互作用力的影响：强力和电磁力。强力的作用范围非常小，主要限制在原子核内，具有极强的吸引作用，使核子（质子和中子）紧密结合在一起。电磁力的作用范围较大，主要通过核外的电子与原子核相互作用，发挥作用。\\n\\n这就是原子的基本结构。原子内部结构复杂多样，不同元素的原子核中质子、中子数量不同，核外电子排布分布也不同，形成了丰富多彩的化学世界。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds)\n",
    "print(ds.column_names)\n",
    "ds['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 48818\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds['train'].map(process_func)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 描述原子的结构。\\n\\nAssistant: 原子是物质的基本单位，它由三种基本粒子组成：质子、中子和电子。质子和中子形成原子核，位于原子中心，核外的电子围绕着原子核运动。\\n\\n原子结构具有层次性。原子核中，质子带正电，中子不带电（中性）。原子核非常小且致密，占据了原子总质量的绝大部分。电子带负电，通常围绕核运动，形成若干层次，称为壳层或电子层。电子数量与质子数量相等，使原子呈电中性。\\n\\n电子在每个壳层中都呈规律分布，并且不同壳层所能容纳的电子数也不同。在最里面的壳层一般只能容纳2个电子，其次一层最多可容纳8个电子，再往外的壳层可容纳的电子数逐层递增。\\n\\n原子核主要受到两种相互作用力的影响：强力和电磁力。强力的作用范围非常小，主要限制在原子核内，具有极强的吸引作用，使核子（质子和中子）紧密结合在一起。电磁力的作用范围较大，主要通过核外的电子与原子核相互作用'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[2][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在以发光为基础的显示设备中（如电视、计算机显示器、智能手机和平板电脑显示屏）, 三原色可混合产生大量色彩。其中红色和绿色可以混合生成黄色，红色和蓝色可以混合生成品红色，蓝色和绿色可以混合生成青色。当红色、绿色和蓝色按相等比例混合时，可以产生白色或灰色。\\n\\n此外，在印刷和绘画中，三原色指的是以颜料为基础的红、黄和蓝颜色（RYB）。这三种颜色用以通过减色混合原理来创建色彩。不过，三原色的具体定义并不唯一，不同的颜色系统可能会采用不同的三原色。<|im_end|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_ds[1]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.q_proj.bias\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.k_proj.bias\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.v_proj.bias\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.q_proj.bias\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.k_proj.bias\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.v_proj.bias\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.q_proj.bias\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.k_proj.bias\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.v_proj.bias\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.q_proj.bias\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.k_proj.bias\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.v_proj.bias\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.q_proj.bias\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.k_proj.bias\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.v_proj.bias\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.q_proj.bias\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.k_proj.bias\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.v_proj.bias\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.q_proj.bias\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.k_proj.bias\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.v_proj.bias\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.q_proj.bias\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.k_proj.bias\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.v_proj.bias\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.q_proj.bias\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.k_proj.bias\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.v_proj.bias\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.q_proj.bias\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.k_proj.bias\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.v_proj.bias\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.q_proj.bias\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.k_proj.bias\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.v_proj.bias\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.q_proj.bias\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.k_proj.bias\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.v_proj.bias\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.q_proj.bias\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.k_proj.bias\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.v_proj.bias\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.q_proj.bias\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.k_proj.bias\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.v_proj.bias\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.q_proj.bias\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.k_proj.bias\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.v_proj.bias\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.q_proj.bias\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.k_proj.bias\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.v_proj.bias\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.q_proj.bias\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.k_proj.bias\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.v_proj.bias\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.q_proj.bias\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.k_proj.bias\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.v_proj.bias\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.q_proj.bias\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.k_proj.bias\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.v_proj.bias\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.q_proj.bias\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.k_proj.bias\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.v_proj.bias\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.q_proj.bias\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.k_proj.bias\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.v_proj.bias\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.q_proj.bias\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.k_proj.bias\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.v_proj.bias\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.q_proj.bias\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.k_proj.bias\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.v_proj.bias\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.q_proj.bias\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.k_proj.bias\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.v_proj.bias\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.q_proj.bias\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.k_proj.bias\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.v_proj.bias\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.q_proj.bias\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.k_proj.bias\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.v_proj.bias\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.q_proj.bias\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.k_proj.bias\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.v_proj.bias\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.q_proj.bias\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.k_proj.bias\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.v_proj.bias\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lora参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=16, target_modules={'q_proj', 'v_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=['q_proj', 'v_proj'], r=16, lora_alpha=16)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, config)  \n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.0.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight\n",
      "base_model.model.model.layers.0.input_layernorm.weight\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.1.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight\n",
      "base_model.model.model.layers.1.input_layernorm.weight\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.2.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight\n",
      "base_model.model.model.layers.2.input_layernorm.weight\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.3.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight\n",
      "base_model.model.model.layers.3.input_layernorm.weight\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.4.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight\n",
      "base_model.model.model.layers.4.input_layernorm.weight\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.5.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight\n",
      "base_model.model.model.layers.5.input_layernorm.weight\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.6.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight\n",
      "base_model.model.model.layers.6.input_layernorm.weight\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.7.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight\n",
      "base_model.model.model.layers.7.input_layernorm.weight\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.8.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight\n",
      "base_model.model.model.layers.8.input_layernorm.weight\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.9.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight\n",
      "base_model.model.model.layers.9.input_layernorm.weight\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.10.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight\n",
      "base_model.model.model.layers.10.input_layernorm.weight\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.11.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight\n",
      "base_model.model.model.layers.11.input_layernorm.weight\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.12.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight\n",
      "base_model.model.model.layers.12.input_layernorm.weight\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.13.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight\n",
      "base_model.model.model.layers.13.input_layernorm.weight\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.14.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight\n",
      "base_model.model.model.layers.14.input_layernorm.weight\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.15.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight\n",
      "base_model.model.model.layers.15.input_layernorm.weight\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.16.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight\n",
      "base_model.model.model.layers.16.input_layernorm.weight\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.17.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight\n",
      "base_model.model.model.layers.17.input_layernorm.weight\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.18.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight\n",
      "base_model.model.model.layers.18.input_layernorm.weight\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.19.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight\n",
      "base_model.model.model.layers.19.input_layernorm.weight\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.20.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight\n",
      "base_model.model.model.layers.20.input_layernorm.weight\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.21.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight\n",
      "base_model.model.model.layers.21.input_layernorm.weight\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.22.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight\n",
      "base_model.model.model.layers.22.input_layernorm.weight\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.23.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight\n",
      "base_model.model.model.layers.23.input_layernorm.weight\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.24.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight\n",
      "base_model.model.model.layers.24.input_layernorm.weight\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.25.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight\n",
      "base_model.model.model.layers.25.input_layernorm.weight\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.26.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight\n",
      "base_model.model.model.layers.26.input_layernorm.weight\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.bias\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.27.self_attn.k_proj.bias\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.bias\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight\n",
      "base_model.model.model.layers.27.input_layernorm.weight\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight\n",
      "base_model.model.model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in lora_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lora模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/data01/tqbian/src/learning/LLM_Toturials/LoRa_Qwen2.5/output_model/\",  \n",
    "    per_device_train_batch_size=32,  \n",
    "    gradient_accumulation_steps=8,  \n",
    "    logging_steps=50,  \n",
    "    num_train_epochs=3 \n",
    "    \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 36:32, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.730400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.538700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.534600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.538100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/tqbian/miniconda3/envs/chatchat/lib/python3.9/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data01/tqbian/modelPATH/Qwen/Qwen2___5-1___5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data01/tqbian/miniconda3/envs/chatchat/lib/python3.9/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data01/tqbian/modelPATH/Qwen/Qwen2___5-1___5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=570, training_loss=1.5643828977618301, metrics={'train_runtime': 2196.3415, 'train_samples_per_second': 66.681, 'train_steps_per_second': 0.26, 'total_flos': 2.941015137760051e+17, 'train_loss': 1.5643828977618301, 'epoch': 2.9882044560943646})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6f55aa49d0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABG6ElEQVR4nO3de1xUdf4/8NdhgOE6wx0G5CYqoCaSKKmplCRSXza+dtmyVlO3dsvalHJbt8223XYtv7ut666b26bZRdu2C6z121wVFTRRU5vMTJSLAnK/zcAAw2XO7w9gdAKVUYYzl9fz0XnonHPmzHvGdF6cz00QRVEEERERkY1zkroAIiIiouHAUENERER2gaGGiIiI7AJDDREREdkFhhoiIiKyCww1REREZBcYaoiIiMguMNQQERGRXXCWuoCRYjAYUFlZCW9vbwiCIHU5RERENASiKKKlpQWhoaFwcrr6vRiHCTWVlZUIDw+XugwiIiK6DuXl5Rg1atRVz3GYUOPt7Q2g90NRKBQSV0NERERDodVqER4ebvwevxqHCTX9TU4KhYKhhoiIyMYMpesIOwoTERGRXWCoISIiIrvAUENERER2gaGGiIiI7AJDDREREdkFhhoiIiKyCww1REREZBcYaoiIiMguMNQQERGRXWCoISIiIrvAUENERER2gaGGiIiI7ILDLGhpKVWadnzwZTnau3qwOj1e6nKIiIgcFu/U3KCG1k6s33MO7xy6gI6uHqnLISIiclgMNTdoQqgCIQo3tHf1oKC4QepyiIiIHBZDzQ0SBAG3xwcBAPZ8VyNxNURERI6LoWYYpPaFmr1naiGKosTVEBEROSaGmmEwIyYAbi5OqNJ04HSVVupyiIiIHJLZoSY/Px8ZGRkIDQ2FIAjIycm56vmPPPIIBEEYsE2YMMF4ztq1azF16lR4e3sjKCgImZmZKCwsNLlOSkrKgGv89Kc/Nbd8i3BzkeHWMYEAgNzvaiWuhoiIyDGZHWp0Oh0SEhKwcePGIZ3/5z//GVVVVcatvLwcfn5+uO+++4zn5OXlYfny5Th8+DB2796Nrq4uzJs3DzqdzuRajz76qMm11q1bZ275FtPfBJXLfjVERESSMHuemvT0dKSnpw/5fKVSCaVSaXyck5ODpqYmLFmyxLhv586dJs/ZunUrgoKCcPz4ccyePdu438PDAyEhIeaWPCJuj+sNNV9XaFCr7UCQwk3iioiIiBzLiPep2bx5M1JTUxEZGXnFczQaDQDAz8/PZP+2bdsQEBCAiRMnYvXq1Whra7NoreYIUrghYVRveNt7hk1QREREI21EZxSurKzE559/ju3bt1/xHIPBgBUrVmDmzJmYOHGicf/ChQsRGRmJ0NBQnDx5Es899xwKCwvxySefDHodvV4PvV5vfKzVWr4D79z4YHxdocGe72rxwLQIi78eERERXTKioebtt9+Gj48PMjMzr3jO8uXLcerUKRw8eNBk/2OPPWb8/U033QSVSoW5c+eiuLgYMTExA66zdu1avPTSS8NW+1DMjQ/Ca7vP4mBRHTq6euDmIhvR1yciInJkI9b8JIoitmzZgh/96EdwdXUd9Jwnn3wSn332Gfbt24dRo0Zd9XrJyckAgKKiokGPr169GhqNxriVl5ff2BsYgvEqBUKVbujoMuBQcb3FX4+IiIguGbFQk5eXh6KiIixbtmzAMVEU8eSTTyI7Oxt79+5FdHT0Na+nVqsBACqVatDjcrkcCoXCZLM009mF2a+GiIhoJJkdalpbW6FWq42horS0FGq1GmVlZQB675AsWrRowPM2b96M5ORkk34y/ZYvX4733nsP27dvh7e3N6qrq1FdXY329nYAQHFxMX7729/i+PHjOH/+PHbs2IFFixZh9uzZmDRpkrlvwaLmxgcDAPZ+x9mFiYiIRpLZoebYsWNITExEYmIiACArKwuJiYlYs2YNAKCqqsoYcPppNBp8/PHHg96lAYDXX38dGo0GKSkpUKlUxu2DDz4AALi6umLPnj2YN28e4uLi8Mwzz+Cee+7Bp59+am75Fjd9tD88XGWo1nbg20rOLkxERDRSBNFBbidotVoolUpoNBqLN0U99s4x7DpdgxWpY7EidZxFX4uIiMiemfP9zbWfLCC1rwmKSyYQERGNHIYaC7gtLgiCAHxzUYMabYfU5RARETkEhhoLCPSWI2GUDwDerSEiIhopDDUWwgUuiYiIRhZDjYX0D+0+WFSP9s4eiashIiKyfww1FhIX4o0wH3fouw34ooizCxMREVkaQ42FCIKAuf1NUGfYBEVERGRpDDUWdHtcf7+aWhgMDjEdEBERkWQYaizolr7ZhWtb9DhVqZG6HCIiIrvGUGNBbi4yzBobAIALXBIREVkaQ42FzTXOLsx+NURERJbEUGNht/fNLvxtpRZVmnapyyEiIrJbDDUWFuAlx+RwHwCcXZiIiMiSGGpGQCqboIiIiCyOoWYE9M9X80VxA9o6uyWuhoiIyD4x1IyA2ODe2YU7uw34oqhB6nKIiIjsEkPNCBAEgQtcEhERWRhDzQgxDu0+w9mFiYiILIGhZoQkj/aDp6sMdS16fHORswsTERENN4aaESJ3lmH2uEAAbIIiIiKyBIaaEdTfBMUlE4iIiIYfQ80Iui02EIIAnK7SorKZswsTERENJ4aaEeTvJcfNEb4AejsMExER0fBhqBlhczm0m4iIyCIYakZY/5IJhzi7MBER0bBiqBlhY4O8EO7XO7vwgXP1UpdDRERkNxhqRpggCJgbxwUuiYiIhhtDjQT6m6D2nqnj7MJERETDhKFGAtOi/eAtd0Z9qx5fVzRLXQ4REZFdYKiRgKuz02WzC3NoNxER0XBgqJFI/9DuPexXQ0RENCwYaiRyW2wQnATgTHULKprapC6HiIjI5pkdavLz85GRkYHQ0FAIgoCcnJyrnv/II49AEIQB24QJE0zO27hxI6KiouDm5obk5GQcPXrU5HhHRweWL18Of39/eHl54Z577kFNje3e5fD1dMWUyN7ZhfdydmEiIqIbZnao0el0SEhIwMaNG4d0/p///GdUVVUZt/Lycvj5+eG+++4znvPBBx8gKysLL774Ik6cOIGEhASkpaWhtvbSl/3KlSvx6aef4sMPP0ReXh4qKyuxYMECc8u3KrfHcYFLIiKi4SKIonjdY4oFQUB2djYyMzOH/JycnBwsWLAApaWliIyMBAAkJydj6tSp+Otf/woAMBgMCA8Px1NPPYVf/OIX0Gg0CAwMxPbt23HvvfcCAM6cOYP4+HgUFBTglltuuebrarVaKJVKaDQaKBQK89+sBZyracEdf8qHq8wJJ9bcAS+5s9QlERERWRVzvr9HvE/N5s2bkZqaagw0nZ2dOH78OFJTUy8V5eSE1NRUFBQUAACOHz+Orq4uk3Pi4uIQERFhPOf79Ho9tFqtyWZtxgR5IcLPA509Bhw8Vyd1OURERDZtRENNZWUlPv/8c/z4xz827quvr0dPTw+Cg4NNzg0ODkZ1dTUAoLq6Gq6urvDx8bniOd+3du1aKJVK4xYeHj68b2YYCIJw2SgoNkERERHdiBENNW+//TZ8fHzMaq66XqtXr4ZGozFu5eXlFn/N69E/u/C+M7Xo4ezCRERE123EQo0oitiyZQt+9KMfwdXV1bg/ICAAMplswEimmpoahISEAABCQkLQ2dmJ5ubmK57zfXK5HAqFwmSzRlOjemcXbtB1Ql3eLHU5RERENmvEQk1eXh6KioqwbNkyk/2urq6YMmUKcnNzjfsMBgNyc3Mxffp0AMCUKVPg4uJick5hYSHKysqM59gqV2cnzI7tn13YdoeoExERSc3sUNPa2gq1Wg21Wg0AKC0thVqtRllZGYDeZp9FixYNeN7mzZuRnJyMiRMnDjiWlZWFf/zjH3j77bfx3Xff4fHHH4dOp8OSJUsAAEqlEsuWLUNWVhb27duH48ePY8mSJZg+ffqQRj5Zu9S+fjVcMoGIiOj6mT2G+NixY7jtttuMj7OysgAAixcvxtatW1FVVWUMOP00Gg0+/vhj/PnPfx70mj/84Q9RV1eHNWvWoLq6GpMnT8bOnTtNOg//6U9/gpOTE+655x7o9XqkpaXhb3/7m7nlW6WUcb2zCxfWtKC8sQ3hfh5Sl0RERGRzbmieGltijfPUXO7+TQU4er4RL/1gAhbPiJK6HCIiIqtg1fPU0OC4wCUREdGNYaixEnP7hnYfKWlEq75b4mqIiIhsD0ONlYgJ9ESUf+/swgfOcnZhIiIiczHUWIne2YW5wCUREdH1YqixIv39avYVcnZhIiIiczHUWJGpUX7wdnNGo64T6vImqcshIiKyKQw1VsRF5oSUWC5wSUREdD0YaqzMpdmFObSbiIjIHAw1ViZlXBBkTgLO1rSivLFN6nKIiIhsBkONlVF6uCAp0hcAJ+IjIiIyB0ONFUrtG9rNBS6JiIiGjqHGCvUP7T5S2oCWji6JqyEiIrINDDVWaHSgF0YHeKKrR0T+2XqpyyEiIrIJDDVWai5HQREREZmFocZK9S+ZwNmFiYiIhoahxkolRfpC6e6CprYunCjj7MJERETXwlBjpZxlTkiJDQTAod1ERERDwVBjxW6P6+9Xw6HdRERE18JQY8X6Zxcuqm3FhQad1OUQERFZNYYaK6b0cMHUqP7ZhXm3hoiI6GoYaqzcpdmF2a+GiIjoahhqrFz/0O6jpY3QcnZhIiKiK2KosXLRAZ4YHeiJboOIvMI6qcshIiKyWgw1NoBNUERERNfGUGMD5vYN7d5XWIfuHoPE1RAREVknhhobMKVvdmFNexeOX+DswkRERINhqLEBzjIn3NY3u3DuGQ7tJiIiGgxDjY2Yy341REREV8VQYyPmxAbC2UlAcZ0O5+s5uzAREdH3MdTYCIWbC6ZF+wHgApdERESDYaixIZeaoNivhoiI6PvMDjX5+fnIyMhAaGgoBEFATk7ONZ+j1+vx/PPPIzIyEnK5HFFRUdiyZYvxeEpKCgRBGLDdddddxnMeeeSRAcfnz59vbvk2LTW+d2j3l+cboWnn7MJERESXczb3CTqdDgkJCVi6dCkWLFgwpOfcf//9qKmpwebNmzFmzBhUVVXBYLg038onn3yCzs5O4+OGhgYkJCTgvvvuM7nO/Pnz8dZbbxkfy+Vyc8u3aZH+nhgT5IWi2lbkna3DDxJCpS6JiIjIapgdatLT05Genj7k83fu3Im8vDyUlJTAz6+3T0hUVJTJOf37+/3zn/+Eh4fHgFAjl8sREhJibsl2ZW58EIpqW5H7XQ1DDRER0WUs3qdmx44dSEpKwrp16xAWFoZx48bh2WefRXt7+xWfs3nzZjzwwAPw9PQ02b9//34EBQUhNjYWjz/+OBoaGq54Db1eD61Wa7LZg/4lE/ZzdmEiIiITZt+pMVdJSQkOHjwINzc3ZGdno76+Hk888QQaGhpMmpL6HT16FKdOncLmzZtN9s+fPx8LFixAdHQ0iouL8ctf/hLp6ekoKCiATCYbcJ21a9fipZdestj7ksrNEb7w9XBBU1sXjl1owi2j/aUuiYiIyCoIoiiK1/1kQUB2djYyMzOveM68efNw4MABVFdXQ6lUAujtQ3PvvfdCp9PB3d3d5Pyf/OQnKCgowMmTJ6/62iUlJYiJicGePXswd+7cAcf1ej30er3xsVarRXh4ODQaDRQKhRnv0vpkfaDGJ19dxKOzovH8XeOlLoeIiMhitFotlErlkL6/Ld78pFKpEBYWZgw0ABAfHw9RFFFRUWFyrk6nwz//+U8sW7bsmtcdPXo0AgICUFRUNOhxuVwOhUJhstkLDu0mIiIayOKhZubMmaisrERra6tx39mzZ+Hk5IRRo0aZnPvhhx9Cr9fj4YcfvuZ1Kyoq0NDQAJVKNew1W7vZ4wLgIhNQUq9DSV3rtZ9ARETkAMwONa2trVCr1VCr1QCA0tJSqNVqlJWVAQBWr16NRYsWGc9fuHAh/P39sWTJEpw+fRr5+flYtWoVli5dOqDpafPmzcjMzIS/v2k/kdbWVqxatQqHDx/G+fPnkZubi7vvvhtjxoxBWlqauW/B5nm7uSA5uvcz4t0aIiKiXmaHmmPHjiExMRGJiYkAgKysLCQmJmLNmjUAgKqqKmPAAQAvLy/s3r0bzc3NSEpKwkMPPYSMjAxs2LDB5LqFhYU4ePDgoE1PMpkMJ0+exA9+8AOMGzcOy5Ytw5QpU3DgwAGHm6um39y+ifi4ZAIREVGvG+oobEvM6WhkC8ob2zBr3T7InASc+NUdUHq4SF0SERHRsLOqjsJkGeF+HhgX7IUeg4j9Z9kERURExFBjw26P6x0FtYf9aoiIiBhqbFn/Apf7C2vRxdmFiYjIwTHU2LDECF/4ebqipaMbX55vlLocIiIiSTHU2DCZk4CU2EAAHNpNRETEUGPjUo2zC9fAQQayERERDYqhxsbNGts7u/D5hjYU1+mkLoeIiEgyDDU2ztvNxbhSdy4n4iMiIgfGUGMH5sb1joJivxoiInJkDDV2oH/V7mMXGtGk65S4GiIiImkw1NiBcD8PxAZ7wyCCswsTEZHDYqixE5cWuGSoISIix8RQYyf6m6DyC+s4uzARETkkhho7MTncB/6ermjRd+PLUs4uTEREjoehxk7InATcFscmKCIiclwMNXakf4HL3DOcXZiIiBwPQ40dmTU2EK4yJ1xoaENxXavU5RAREY0ohho74il3xi0xvbMLswmKiIgcDUONnTE2QXHJBCIicjAMNXbm9r7OwscvNHF2YSIicigMNXZmlK8H4kJ6ZxfeV8gmKCIichwMNXYotW8iPi5wSUREjoShxg71L5mQd7YOnd2cXZiIiBwDQ40dShjlgwAvOVr13TjK2YWJiMhBMNTYIScnAbfHBQIA9nAUFBEROQiGGjvVv8AlZxcmIiJHwVBjp2aNDYCrsxPKG9txrpazCxMRkf1jqLFTHq7OmGGcXZhNUEREZP8YauzY3Lj+2YU5tJuIiOwfQ40du72vX82JsiY0tOolroaIiMiyGGrsWJiPO+JVCogisK+wTupyiIiILMrsUJOfn4+MjAyEhoZCEATk5ORc8zl6vR7PP/88IiMjIZfLERUVhS1bthiPb926FYIgmGxubm4m1xBFEWvWrIFKpYK7uztSU1Nx7tw5c8t3OFzgkoiIHIXZoUan0yEhIQEbN24c8nPuv/9+5ObmYvPmzSgsLMT777+P2NhYk3MUCgWqqqqM24ULF0yOr1u3Dhs2bMCmTZtw5MgReHp6Ii0tDR0dHea+BYfSP7Q7/2wd9N09EldDRERkOc7mPiE9PR3p6elDPn/nzp3Iy8tDSUkJ/Pz8AABRUVEDzhMEASEhIYNeQxRFrF+/Hr/61a9w9913AwDeeecdBAcHIycnBw888IC5b8NhTApTItBbjroWPY6UNGL2uECpSyIiIrIIi/ep2bFjB5KSkrBu3TqEhYVh3LhxePbZZ9He3m5yXmtrKyIjIxEeHo67774b3377rfFYaWkpqqurkZqaatynVCqRnJyMgoKCQV9Xr9dDq9WabI7IyUnA7bFsgiIiIvtn8VBTUlKCgwcP4tSpU8jOzsb69evx0Ucf4YknnjCeExsbiy1btuDf//433nvvPRgMBsyYMQMVFRUAgOrqagBAcHCwybWDg4ONx75v7dq1UCqVxi08PNxC79D69S9wuee7Ws4uTEREdsviocZgMEAQBGzbtg3Tpk3DnXfeiddeew1vv/228W7N9OnTsWjRIkyePBlz5szBJ598gsDAQPz973+/7tddvXo1NBqNcSsvLx+ut2Rzbu2bXfhiczsKa1qkLoeIiMgiLB5qVCoVwsLCoFQqjfvi4+MhiqLxTsz3ubi4IDExEUVFRQBg7GtTU2PafFJTU3PFfjhyuRwKhcJkc1Qers6Y2Te7MCfiIyIie2XxUDNz5kxUVlaitfXS+kNnz56Fk5MTRo0aNehzenp68M0330ClUgEAoqOjERISgtzcXOM5Wq0WR44cwfTp0y37BuxE/ygoLplARET2yuxQ09raCrVaDbVaDaC3E69arUZZWRmA3mafRYsWGc9fuHAh/P39sWTJEpw+fRr5+flYtWoVli5dCnd3dwDAb37zG+zatQslJSU4ceIEHn74YVy4cAE//vGPAfSOjFqxYgVefvll7NixA9988w0WLVqE0NBQZGZm3uBH4Bj6+9Woy5tRz9mFiYjIDpk9pPvYsWO47bbbjI+zsrIAAIsXL8bWrVtRVVVlDDgA4OXlhd27d+Opp55CUlIS/P39cf/99+Pll182ntPU1IRHH30U1dXV8PX1xZQpU3Do0CGMHz/eeM7Pf/5z6HQ6PPbYY2hubsatt96KnTt3DpikjwanUrpjQqgC31Zqse9MLe5LctyO00REZJ8E0UGGw2i1WiiVSmg0GoftX/Pa7rPYkHsO8yeEYNOPpkhdDhER0TWZ8/3NtZ8cSP+SCQfOcXZhIiKyPww1DmRiqBJB3nLoOntwuKRR6nKIiIiGFUONA3FyEowdhjm7MBER2RuGGgczN653aHcuZxcmIiI7w1DjYGaOCYC8b3bhM9WcXZiIiOwHQ42DcXeV4dYxAQDYBEVERPaFocYBXZpdmEsmEBGR/WCocUCXzy5cq+2QuBoiIqLhwVDjgIIVbkgI9wEA7DrNJigiIrIPDDUOKm1CbxPUf7+tlrgSIiKi4cFQ46DSJoQAAAqKG6Bp75K4GiIiohvHUOOgYgK9MCbIC90GEfvOsMMwERHZPoYaB8YmKCIisicMNQ5s/gQVAGB/YR06urjAJRER2TaGGgc2MUyBMB93tHf1IP9sndTlEBER3RCGGgcmCALuGN/fBMWh3UREZNsYahxc/yio3DM16O4xSFwNERHR9WOocXBTo3zh6+GC5rYuHC1tlLocIiKi68ZQ4+CcZU5IjecoKCIisn0MNYT5E3uboP77bQ0MBlHiaoiIiK4PQw1h5pgAeLrKUK3twMmLGqnLISIiui4MNQQ3FxlSYntX7mYTFBER2SqGGgIAzOPswkREZOMYaggAcFtcEFxkAkrqdCiqbZG6HCIiIrMx1BAAQOHmghkxAQA4ER8REdkmhhoyujQKik1QRERkexhqyCg1PhiCAJys0OBic7vU5RAREZmFoYaMAr3lSIr0BQDs4t0aIiKyMQw1ZKJ/LSg2QRERka1hqCET/aHmaGkjGnWdEldDREQ0dAw1ZCLczwPjVQoYRGDPdxwFRUREtoOhhgbov1vDfjVERGRLzA41+fn5yMjIQGhoKARBQE5OzjWfo9fr8fzzzyMyMhJyuRxRUVHYsmWL8fg//vEPzJo1C76+vvD19UVqaiqOHj1qco1HHnkEgiCYbPPnzze3fBqCtIm9swvnn6tHq75b4mqIiIiGxuxQo9PpkJCQgI0bNw75Offffz9yc3OxefNmFBYW4v3330dsbKzx+P79+/Hggw9i3759KCgoQHh4OObNm4eLFy+aXGf+/Pmoqqoybu+//7655dMQxAZ7I9LfA53dBuQV1kldDhER0ZA4m/uE9PR0pKenD/n8nTt3Ii8vDyUlJfDz8wMAREVFmZyzbds2k8dvvvkmPv74Y+Tm5mLRokXG/XK5HCEhIeaWTGYSBAFpE0LwRn4J/vttNe6apJK6JCIiomuyeJ+aHTt2ICkpCevWrUNYWBjGjRuHZ599Fu3tV57cra2tDV1dXcYQ1G///v0ICgpCbGwsHn/8cTQ0NFzxGnq9Hlqt1mSjoUvrW+By35ladHYbJK6GiIjo2sy+U2OukpISHDx4EG5ubsjOzkZ9fT2eeOIJNDQ04K233hr0Oc899xxCQ0ORmppq3Dd//nwsWLAA0dHRKC4uxi9/+Uukp6ejoKAAMplswDXWrl2Ll156yWLvy94lhvsi0FuOuhY9DhXXIyU2SOqSiIiIrkoQRVG87icLArKzs5GZmXnFc+bNm4cDBw6guroaSqUSAPDJJ5/g3nvvhU6ng7u7u8n5r7zyCtatW4f9+/dj0qRJV7xuSUkJYmJisGfPHsydO3fAcb1eD71eb3ys1WoRHh4OjUYDhUJh5jt1TM9nf4NtR8rw4LQIrF1wk9TlEBGRA9JqtVAqlUP6/rZ485NKpUJYWJgx0ABAfHw8RFFERUWFybl/+MMf8Morr2DXrl1XDTQAMHr0aAQEBKCoqGjQ43K5HAqFwmQj8/QP7d59uho9huvOvkRERCPC4qFm5syZqKysRGtrq3Hf2bNn4eTkhFGjRhn3rVu3Dr/97W+xc+dOJCUlXfO6FRUVaGhogErFTqyWcstof3i7OaO+tRMnypqkLoeIiOiqzA41ra2tUKvVUKvVAIDS0lKo1WqUlZUBAFavXm0yYmnhwoXw9/fHkiVLcPr0aeTn52PVqlVYunSpsenp1VdfxQsvvIAtW7YgKioK1dXVqK6uNgah1tZWrFq1CocPH8b58+eRm5uLu+++G2PGjEFaWtqNfgZ0Ba7OTpgb19uX5r+nOBEfERFZN7NDzbFjx5CYmIjExEQAQFZWFhITE7FmzRoAQFVVlTHgAICXlxd2796N5uZmJCUl4aGHHkJGRgY2bNhgPOf1119HZ2cn7r33XqhUKuP2hz/8AQAgk8lw8uRJ/OAHP8C4ceOwbNkyTJkyBQcOHIBcLr+hD4CuzrjA5elq3ED3KyIiIou7oY7CtsScjkZ0SVtnNxJ/sxv6bgP+87NZGB/Kz46IiEaOVXUUJtvm4eqM2eMCAQD/5VpQRERkxRhq6JqMTVAMNUREZMUYauiaUuODIHMScKa6BRcadFKXQ0RENCiGGromHw9XJEf3LlnBuzVERGStGGpoSC41QdVIXAkREdHgGGpoSOb1LXB5oqwJtS0dEldDREQ0EEMNDYlK6Y6EcB+IIrD7NO/WEBGR9WGooSFL67tbs5OzCxMRkRViqKEh6+9XU1DcAE17l8TVEBERmWKooSGLCfTCmCAvdBtE7DtTK3U5REREJhhqyCz9TVAc2k1ERNaGoYbMMn+CCgCwv7AOHV09EldDRER0CUMNmWVimAJhPu5o7+rBgXP1UpdDRERkxFBDZhEEAXeMZxMUERFZH4YaMlv/KKg939Wgu8cgcTVERES9GGrIbFOjfOHr4YLmti4cLW2UuhwiIiIADDV0HZxlTkiNZxMUERFZF4Yaui7zJ/Y2Qe06XQNRFCWuhoiIiKGGrtPMMQHwdJWhStOBkxUaqcshIiJiqKHr4+YiQ0psEAA2QRERkXVgqKHrNq9/gUuGGiIisgIMNXTdbosLgotMQEmdDkW1LVKXQ0REDo6hhq6bws0FM2ICAAD//bZG4mqIiMjRMdTQDekfBcV+NUREJDWGGrohqfHBEATgZIUGlc3tUpdDREQOjKGGbkigtxxJkb4AgF28W0NERBJiqKEb1r8WFEdBERGRlBhq6Ib1h5qjpY1o1HVKXA0RETkqhhq6YeF+HhivUsAg9q7cTUREJAWGGhoW/Xdr2K+GiIikwlBDwyJtYu/swvnn6qHTd0tcDREROSKzQ01+fj4yMjIQGhoKQRCQk5Nzzefo9Xo8//zziIyMhFwuR1RUFLZs2WJyzocffoi4uDi4ubnhpptuwn/+8x+T46IoYs2aNVCpVHB3d0dqairOnTtnbvlkIbHB3oj090BntwF5Z+ukLoeIiByQ2aFGp9MhISEBGzduHPJz7r//fuTm5mLz5s0oLCzE+++/j9jYWOPxQ4cO4cEHH8SyZcvw1VdfITMzE5mZmTh16pTxnHXr1mHDhg3YtGkTjhw5Ak9PT6SlpaGjo8Pct0AWIAjCpVFQp9gERUREI08QRVG87icLArKzs5GZmXnFc3bu3IkHHngAJSUl8PPzG/ScH/7wh9DpdPjss8+M+2655RZMnjwZmzZtgiiKCA0NxTPPPINnn30WAKDRaBAcHIytW7figQceuGatWq0WSqUSGo0GCoXCvDdKQ3L8QiPueb0A3nJnHH/hDrg6s3WTiIhujDnf3xb/1tmxYweSkpKwbt06hIWFYdy4cXj22WfR3n5p9tmCggKkpqaaPC8tLQ0FBQUAgNLSUlRXV5uco1QqkZycbDzn+/R6PbRarclGlpUY7otAbzla9N04VFwvdTlERORgLB5qSkpKcPDgQZw6dQrZ2dlYv349PvroIzzxxBPGc6qrqxEcHGzyvODgYFRXVxuP9++70jnft3btWiiVSuMWHh4+nG+LBuHkJGDe+N4/Iy5wSUREI83iocZgMEAQBGzbtg3Tpk3DnXfeiddeew1vv/22yd2a4bZ69WpoNBrjVl5ebrHXokv6+9XsPl2DHsN1t2wSERGZzeKhRqVSISwsDEql0rgvPj4eoiiioqICABASEoKaGtOf7GtqahASEmI83r/vSud8n1wuh0KhMNnI8m4Z7Q9vN2fUt+rxVVmT1OUQEZEDsXiomTlzJiorK9Ha2mrcd/bsWTg5OWHUqFEAgOnTpyM3N9fkebt378b06dMBANHR0QgJCTE5R6vV4siRI8ZzyDq4OjthblwQAI6CIiKikWV2qGltbYVarYZarQbQ24lXrVajrKwMQG+zz6JFi4znL1y4EP7+/liyZAlOnz6N/Px8rFq1CkuXLoW7uzsA4Omnn8bOnTvxxz/+EWfOnMGvf/1rHDt2DE8++SSA3lFWK1aswMsvv4wdO3bgm2++waJFixAaGnrVkVckjf4mqP+ersYNDK4jIiIyi9mh5tixY0hMTERiYiIAICsrC4mJiVizZg0AoKqqyhhwAMDLywu7d+9Gc3MzkpKS8NBDDyEjIwMbNmwwnjNjxgxs374db7zxBhISEvDRRx8hJycHEydONJ7z85//HE899RQee+wxTJ06Fa2trdi5cyfc3Nyu+82TZcyJDYTc2Qnlje34rqpF6nKIiMhB3NA8NbaE89SMrEffOYbdp2vw9NyxWHnHOKnLISIiG2VV89SQYzI2QXGBSyIiGiEMNWQRqfFBkDkJOFPdggsNOqnLISIiB8BQQxbh4+GK5OjeZTF4t4aIiEYCQw1ZzKUmKM4uTERElsdQQxYzb0LvkgknyppQ28LV1ImIyLIYashiVEp3JIT7QBR7l00gIiKyJIYasqi0CVzgkoiIRgZDDVlUf7+aguJ6aNq7JK6GiIjsGUMNWVRMoBfGBHmhq0fEvjO1UpdDRER2jKGGLO5SExSHdhMRkeUw1JDFzZ+gAgDsL6xDR1ePxNUQEZG9Yqghi5sYpkCYjzvau3pw4Fy91OUQEZGdYqghixMEAXeMZxMUERFZFkMNjYj+UVC539Wgu8cgcTVERGSPGGpoREyN8oWvhwua2rpw9Hyj1OUQEZEdYqihEeEsc0JqfF8T1Ck2QRER0fBjqKERM39ibxPUrtM1EEVR4mqIiMjeMNTQiJk5JgCerjJUaTpwskIjdTlERGRnGGpoxLi5yJASGwSAo6CIiGj4MdTQiJrH2YWJiMhCGGpoRN0WFwQXmYDiOh2KalukLoeIiOwIQw2NKIWbC2bEBAAA/vttjcTVEBGRPWGooRHXPwqKTVBERDScGGpoxKXGB0MQgJMVGlQ2t0tdDhER2QmGGhpxgd5yJEX6AgB28W4NERENE4YakkT/WlDsV0NERMOFoYYk0R9qjp5vRKOuU+JqiIjIHjDUkCTC/TwwXqVAj0HEnu94t4aIiG4cQw1Jpv9uDfvVEBHRcGCoIcmkTeydXTj/XD10+m6JqyEiIlvHUEOSiQ32RqS/Bzq7Dcg7Wyd1OUREZOPMDjX5+fnIyMhAaGgoBEFATk7OVc/fv38/BEEYsFVXX2pyiIqKGvSc5cuXG89JSUkZcPynP/2pueWTFREE4bJRUGyCIiKiG+Ns7hN0Oh0SEhKwdOlSLFiwYMjPKywshEKhMD4OCgoy/v7LL79ET0+P8fGpU6dwxx134L777jO5xqOPPorf/OY3xsceHh7mlk9WJm1CMN7IL8He72rR2W2AqzNvHhIR0fUxO9Skp6cjPT3d7BcKCgqCj4/PoMcCAwNNHr/yyiuIiYnBnDlzTPZ7eHggJCTE7Ncm65UY7otAbznqWvQ4VFyPlNigaz+JiIhoECP2Y/HkyZOhUqlwxx134IsvvrjieZ2dnXjvvfewdOlSCIJgcmzbtm0ICAjAxIkTsXr1arS1tV3xOnq9Hlqt1mQj6+PkJGDe+N4Ow5yIj4iIboTFQ41KpcKmTZvw8ccf4+OPP0Z4eDhSUlJw4sSJQc/PyclBc3MzHnnkEZP9CxcuxHvvvYd9+/Zh9erVePfdd/Hwww9f8XXXrl0LpVJp3MLDw4fzbdEw6u9Xs/t0DXoMosTVEBGRrRJEUbzubxFBEJCdnY3MzEyznjdnzhxERETg3XffHXAsLS0Nrq6u+PTTT696jb1792Lu3LkoKipCTEzMgON6vR56vd74WKvVIjw8HBqNxqRvD0mvs9uAKS/vRktHNz766XQkRflJXRIREVkJrVYLpVI5pO9vSXplTps2DUVFRQP2X7hwAXv27MGPf/zja14jOTkZAAa9DgDI5XIoFAqTjayTq7MT5sb19qXhKCgiIrpekoQatVoNlUo1YP9bb72FoKAg3HXXXUO6BoBBr0O25/IFLm/g5iERETkws0c/tba2mtwdKS0thVqthp+fHyIiIrB69WpcvHgR77zzDgBg/fr1iI6OxoQJE9DR0YE333wTe/fuxa5du0yuazAY8NZbb2Hx4sVwdjYtq7i4GNu3b8edd94Jf39/nDx5EitXrsTs2bMxadKk63nfZGXmxAZC7uyEssY2fFfVgvGhvLNGRETmMTvUHDt2DLfddpvxcVZWFgBg8eLF2Lp1K6qqqlBWVmY83tnZiWeeeQYXL16Eh4cHJk2ahD179phcAwD27NmDsrIyLF26dMBrurq6Ys+ePVi/fj10Oh3Cw8Nxzz334Fe/+pW55ZOV8nB1xuxxgdh9ugb//baaoYaIiMx2Qx2FbYk5HY1IGh8dr8CzH36NuBBv7FwxW+pyiIjIClh9R2GiwaTGB0HmJOBMdQvKGq48BxEREdFgGGrIavh4uCI5unc4N0dBERGRuRhqyKpwgUsiIrpeDDVkVeZN6F0y4XhZE2pbOiSuhoiIbAlDDVkVldIdCeE+EMXeZROIiIiGiqGGrE7aBC5wSURE5mOoIavT36+moLge2o4uiashIiJbwVBDVicm0AtjgrzQ1SNi35laqcshIiIbwVBDVulSExRHQRER0dAw1JBVmj+hd6HSfWfq0NHVI3E1RERkCxhqyCpNDFMgzMcd7V09OHCuXupyiIjIBjDUkFUSBAF3jGcTFBERDR1DDVmt/lFQud/VoLvHIHE1RERk7RhqyGpNjfKFr4cLmtq6cPR8o9TlEBGRlWOoIavlLHNCanxvE9QuTsRHRETXwFBDVm3+xEsLXIqiKHE1RERkzRhqyKrNHBMAT1cZqjQdOFmhkbocIiKyYgw1ZNXcXGRIiQ0CwFFQRER0dQw1ZPXmcXZhIiIaAoYasnq3xQXBRSaguE6HotpWqcshIiIrxVBDVk/h5oIZMQEAeLeGiIiujKGGbEL/KKhdDDVERHQFDDVkE1LjgyEIwNcVGryQcwrtnVzkkoiITDHUkE0I9JZjZeo4AMC7hy8g468HcbpSK3FVRERkTRhqyGb8bO5YvLN0GgK95SiqbUXmxi/w5oESGAyclI+IiBhqyMbMHheInU/PQmp8MDp7DHj5/32HxW8dRY22Q+rSiIhIYgw1ZHP8veT4x6Ip+N3/ToSbixMOnKvH/PX57ERMROTgGGrIJgmCgIeSI/HZU7divEqBprYuPPbucfwy+xu0dXZLXR4REUmAoYZs2pggb2Qvn4HHZo8GAGw/Uob/+ctBnLrIdaKIiBwNQw3ZPLmzDL+8Mx7vLUtGsEKOkjod/vdvX+DvecXsRExE5EAYashu3Do2ADufno20CcHo6hGx9vMzeHjzEVRr2ImYiMgRmB1q8vPzkZGRgdDQUAiCgJycnKuev3//fgiCMGCrrr7UqfPXv/71gONxcXEm1+no6MDy5cvh7+8PLy8v3HPPPaipqTG3fLJzvp6u2PTwFLyy4Ca4u8hwqLgBaevzsfNUldSlERGRhZkdanQ6HRISErBx40aznldYWIiqqirjFhQUZHJ8woQJJscPHjxocnzlypX49NNP8eGHHyIvLw+VlZVYsGCBueWTAxAEAQ9Mi8BnP7sVN4UpoWnvwk/fO4HnPjoJnZ6diImI7JWzuU9IT09Henq62S8UFBQEHx+fKxfi7IyQkJBBj2k0GmzevBnbt2/H7bffDgB46623EB8fj8OHD+OWW24xux6yfzGBXvj48Rn4056z2JRXjA+OlePo+Uas/+FkJIT7SF0eERENsxHrUzN58mSoVCrccccd+OKLLwYcP3fuHEJDQzF69Gg89NBDKCsrMx47fvw4urq6kJqaatwXFxeHiIgIFBQUjEj9ZJtcnZ3w3Pw4bPtxMkIUbiit1+Ge1w9h474i9LATMRGRXbF4qFGpVNi0aRM+/vhjfPzxxwgPD0dKSgpOnDhhPCc5ORlbt27Fzp078frrr6O0tBSzZs1CS0sLAKC6uhqurq4D7vQEBweb9M25nF6vh1arNdnIcc2ICcDOFbNw500h6DaI+L//FmLhPw6jsrld6tKIiGiYmN38ZK7Y2FjExsYaH8+YMQPFxcX405/+hHfffRcATJqzJk2ahOTkZERGRuJf//oXli1bdl2vu3btWrz00ks3VjzZFR8PV2xceDM+PF6BX+/4FkdKGzF/fT7WLpiEuyappC6PiIhukCRDuqdNm4aioqIrHvfx8cG4ceOM54SEhKCzsxPNzc0m59XU1FyxH87q1auh0WiMW3l5+bDVT7ZLEATcnxSO//xsFhJGKaHt6Mby7Sew6sOv0cpOxERENk2SUKNWq6FSXfkn49bWVhQXFxvPmTJlClxcXJCbm2s8p7CwEGVlZZg+ffqg15DL5VAoFCYbUb+oAE989PgMPHnbGAgC8OHxCty14QC+KmuSujQiIrpOZjc/tba2mtxlKS0thVqthp+fHyIiIrB69WpcvHgR77zzDgBg/fr1iI6OxoQJE9DR0YE333wTe/fuxa5du4zXePbZZ5GRkYHIyEhUVlbixRdfhEwmw4MPPggAUCqVWLZsGbKysuDn5weFQoGnnnoK06dP58gnum4uMic8mxaLWWMDsPIDNS40tOHeTQVYmToWj6eMgcxJkLpEIiIyg9mh5tixY7jtttuMj7OysgAAixcvxtatW1FVVWUycqmzsxPPPPMMLl68CA8PD0yaNAl79uwxuUZFRQUefPBBNDQ0IDAwELfeeisOHz6MwMBA4zl/+tOf4OTkhHvuuQd6vR5paWn429/+dl1vmuhyyaP98fmK2Xg++xt8drIKf9h1Fvln6/HaDxMwytdD6vKIiGiIBFEUHWJcq1arhVKphEajYVMUDUoURXxy4iLW/PsUdJ098HZzxu/+9yb8ICFU6tKIiByWOd/fXPuJqI8gCLhnyij85+lZSIzwQUtHN372/lfI+kCNlo4uqcsjIqJrYKgh+p5If098+JPp+NncsXASgE++uog7NxzA8QvsRExEZM0YaogG4SxzQtYd4/Cvn0xHmI87yhvbcf/fC7B+z1l09xikLo+IiAbBUEN0FUlRfvh8xSxkTg5Fj0HE+j3n8MM3DqO8sU3q0oiI6HsYaoiuQeHmgvUPJGL9DyfDW+6M4xeakP7nA8j+qkLq0oiI6DIMNURDlJkYhv88PQtTIn3Rqu/Gyg++xtP//ApadiImIrIKDDVEZgj388AHj92ClanjIHMS8G91JdLXH8CX5xulLo2IyOEx1BCZyVnmhKdTx+JfP5mOcD93XGxuxw//XoDXdhWyEzERkYQYaoiu05RIX/znZ7Ow4OYwGERgw94i3Pf3Alxo0EldGhGRQ2KoIboB3m4ueO3+ydjwYCK83ZzxVVkz7vzzAXx0vAIOMlk3EZHV4DIJRMOkoqkNWR98jaN9/Wv+Z5IKv8u8CUoPF4krI2sjiiK+rdTim4sauMqc4CmXwVPuDA9XZ3jJneHhKuv9VS6D3FkmdblEkjLn+5uhhmgY9RhEbMorxmu7z6LHICJU6YYX/mc8/DxdIQIQRUCEiL7/jI9F42MRxr+Qlx/73vHev7WXP2/gdfD9/dd6jcvO95I7Y2qUH0J93Efqo7N7mrYu5J+rw/7COuSfq0Ndi35Iz3ORCSZhx1Pu3BuCXJ37glBfAHJ1NoYjT7kzPPvP/d5+DxcZnLgCPdkQhppBMNTQSFKXN+Ppf36FCw22PUlfdIAnZsT449YxAZge4w8fD1epS7IZBkPv3Zj9hbXYf7YOX5U1wXDZv7YerjJMifSFIAjQ6bt7t85utOl70Krvhr7bcp3OPVxlfUFJdikwGYOS6T4v4x2kvv1uzghVuiPIW85wRAB6f1C62NxuXErm7slhw3p9hppBMNTQSGvVd2PdzjM4eK6+d4cACOhdOLP3V0CAAKHve8Fk/2XHhN6DxmN9lxpwnUvXv+y5V3uN7z3ueyUIAlDbosc3Fc0mX8KCAEwIVWBmTABmjgnA1Cg/uLuyaeRyzW2dyD9Xj/2Ftcg/W4/6VtO7MWODvJASG4iU2CAkRfletWmpu8cAXWcP2jr7Ao++py/49BgD0GD72zq70arvRltnbzhq0/cYzzUM47/2rjInhPm6Y5SvO8L9PHp/9fUwPvb3dIUgMPTYo64eA76r0uLY+SYcv9C7VWs7AABjgrywJ2vOsL4eQ80gGGqIzKNp78KRkgYcKm7AF0X1OFfbanLcVeaExAgfzBwTgJlj/DFplA9cZI419sBgEHGqUoP9hXXYX1gLdblpEPR0lWHGmACkxAZizrhAjPL1kKxWURSh7zYYg07roAHostBkEqAu7Wvp6Ea1tgM910hI7i4yY8AJ93XHKF8PhPv1/errwb5mNkTT1oUTZU04dqERxy804etyDdq7ekzOkTkJmBCqQFKkH351V/yw3sVjqBkEQw3RjanVduBQcQMOFtXjUFE9KjUdJse95M5IjvbDjL6QExvsbZc/qTfpOpF/rg55fX1j6ls7TY6PC/ZCSmwQUsYFIinKD67O9hf0unsMqNJ0oLypDRVN7ahobEN5UzsqmtpQ3tiOmpYOXOubxdvN2eTOzqXg07vPU+48Mm+GTIiiiPMNbTh2vrE3yJxvGvADDQAo3JwxJdIXSVF+uDnCFwnhSni4WubPjKFmEAw1RMOn/x++L4rqcai4HoeKG9DcZrpcRICXK2bE9AacGTEBCPeT7i7FjTAYRHxzse9uzNlafD3I3ZiZYwKQEhuEObGBCGPnaui7e1DZ3IHyxjZj8Clv7AtATW0DguBg/DxdjUFnlJ9p01aYjzvcXNj0ORw6unpw6qIGxy804diFJpy40IQG3cA/n+gAz94QE+mLKZG+iAn0GrE+VQw1g2CoIbIcg0HE6SotviiqxxfFDfiytHHA7ekIPw9jU9X00f7w95JLVO21Neo6caB/pNLZugH/yMeFeGNOX5NSUqR93o2xpLbOblxsakd5352d/js8/QFI037t9dSCvOUDmrZ6g48HVD5uDtcUOlR1LXocv9DUdxemEacuatH5vZnQXZ2dMClMiSlRvpgS0RtipPz7ylAzCIYaopGj7+7BV2XNONQXctTlzQP6YMSrFJgZ44+ZYwMwLcpP0uaGHoOIkxXNfXdj6nCyotmk+cRL7oxb+/vGxAZCpeTdGEvSdnQZ7+xcfoenP/i0dfZc9flOAqBSmnZiDlG4IUghR5C3G4K85fD3kkNm56O3DAYR52pb++7CNOLEhSacH2REZoCXa99dGD/cHOmLiWEKq5ofiaFmEAw1RNJp1XfjaGkDDp5rwKHiepypbjE57uwkIDHCp6+5KgCTw30sfvejoVVvnDfmwLl6NA5yNyYlNggpsYGYEunLn/ythCiKaGq7LPQ0tZn8vqKpHZ1DGA7vJAD+XnIEefdv/aFHjkCT38ut6gv+ato6u6Eua77UlFTWhJaObpNzBAEYF+RtvAuTFOWLCD8Pq+7/xlAzCIYaIutR36rHoeIGHCqqx8GielQ0tZsc93CVYVq0H2bGBGDGGH/EhyhuuP2+xyDi6767MXmFtTh5UWNyN8Zb7oxbx/aPVApCiNLthl6PpGEwiKhv1Q/oy1Oj7UBtix61LXo0tOrNGt7u4+GCQC+5yZ2eQG85ghRul0KRwg1eI3y3sbJvbpj+7XSVdsAdUQ9XGSaH+yAp0hc3R/oiMcIXSnfbGnnGUDMIhhoi61XW0IYviuvxRVE9CoobBvRh8fN0xfQY/745cvyH/JNlfase+Wf778bUoel7nZnjVYreeWPGBeJm3o1xGD0GEQ2t+r6Q04Fa7cDf1/Vt3+9vcjUerjLjXZ9AxWV3gLxNA5GPh4vZd0a6eww4U92CY+cbjR16vz8CEQBClW6YEuWHKRE+SIryQ1yIN5xt/P9rhppBMNQQ2QaDQcSZ6hYc6gs5R0obB/ShCPNxx8wx/pg5JgAzYgIQ6N3bibHHIEJd3oy8vll8v/n+3Rg3Z8waG4CUcb0jlYIVvBtDVyaKIjTtXb2BR9sXer73+7oWPWq1HdBdo5/P5VxlTgjsv9vjPcgdIG83+Hq6oKi2FSf6mpLU5c0D/h7InASMVykwpW9E0pRIX7tc2oShZhAMNUS2qavHgK/Lm/FFUe8kgF+VN6Grx/Sfrdhgb0T6e+Do+cYBQ8vH99+NiQ3CzRE+Nv9TK1knnb67L/BcauaqbelA3eV3gVr0A/7/NIfCzRk3R/aNSIryxeRwH4vNDWNNGGoGwVBDZB/aOrtxtLSxdyLAc/U4XaU1Oa5wc8assb2jlFLGBSKId2PIiui7e3rv7vTd8alr6bj0+LImsAZdJ8J93TEl0q9vkjtfjBnBuWGsCUPNIBhqiOxTo64TBcUNKGtsQ1KULxLDeTeGbJ8oilY9ImkkmfP9bf/3rYjIrvl5uuKuSSqpyyAaVgw014c/zhAREZFdYKghIiIiu8BQQ0RERHaBoYaIiIjsAkMNERER2QWzQ01+fj4yMjIQGhoKQRCQk5Nz1fP3798PQRAGbNXV1cZz1q5di6lTp8Lb2xtBQUHIzMxEYWGhyXVSUlIGXOOnP/2pueUTERGRnTI71Oh0OiQkJGDjxo1mPa+wsBBVVVXGLSgoyHgsLy8Py5cvx+HDh7F79250dXVh3rx50Ol0Jtd49NFHTa6xbt06c8snIiIiO2X2PDXp6elIT083+4WCgoLg4+Mz6LGdO3eaPN66dSuCgoJw/PhxzJ4927jfw8MDISEhZr82ERER2b8R61MzefJkqFQq3HHHHfjiiy+ueq5GowEA+Pn5mezftm0bAgICMHHiRKxevRptbW1XvIZer4dWqzXZiIiIyH5ZfEZhlUqFTZs2ISkpCXq9Hm+++SZSUlJw5MgR3HzzzQPONxgMWLFiBWbOnImJEyca9y9cuBCRkZEIDQ3FyZMn8dxzz6GwsBCffPLJoK+7du1avPTSSxZ7X0RERGRdbmjtJ0EQkJ2djczMTLOeN2fOHERERODdd98dcOzxxx/H559/joMHD2LUqFFXvMbevXsxd+5cFBUVISYmZsBxvV4PvV5vfKzVahEeHs61n4iIiGyIOWs/STKke9q0aSgqKhqw/8knn8Rnn32Gffv2XTXQAEBycjIADHodAJDL5VAoFCYbERER2S9JFrRUq9VQqS4tQCeKIp566ilkZ2dj//79iI6OHtI1AJhch4iIiByX2aGmtbXV5O5IaWkp1Go1/Pz8EBERgdWrV+PixYt45513AADr169HdHQ0JkyYgI6ODrz55pvYu3cvdu3aZbzG8uXLsX37dvz73/+Gt7e3cQ4bpVIJd3d3FBcXY/v27bjzzjvh7++PkydPYuXKlZg9ezYmTZo0pLr7W9nYYZiIiMh29H9vD6m3jGimffv2iQAGbIsXLxZFURQXL14szpkzx3j+q6++KsbExIhubm6in5+fmJKSIu7du9fkmoNdD4D41ltviaIoimVlZeLs2bNFPz8/US6Xi2PGjBFXrVolajSaIdddXl5+xdfhxo0bN27cuFn3Vl5efs3v+hvqKGxLDAYDKisr4e3tDUEQhvXa/Z2Qy8vL2XfHgvg5jwx+ziODn/PI4Wc9Miz1OYuiiJaWFoSGhsLJ6epdgSXpUyMFJyena3Y+vlHskDwy+DmPDH7OI4Of88jhZz0yLPE5K5XKIZ3HBS2JiIjILjDUEBERkV1gqBkGcrkcL774IuRyudSl2DV+ziODn/PI4Oc8cvhZjwxr+JwdpqMwERER2TfeqSEiIiK7wFBDREREdoGhhoiIiOwCQw0RERHZBYaaG7Rx40ZERUXBzc0NycnJOHr0qNQl2Z21a9di6tSp8Pb2RlBQEDIzM1FYWCh1WXbvlVdegSAIWLFihdSl2J2LFy/i4Ycfhr+/P9zd3XHTTTfh2LFjUpdlV3p6evDCCy8gOjoa7u7uiImJwW9/+9uhrR9EV5Wfn4+MjAyEhoZCEATk5OSYHBdFEWvWrIFKpYK7uztSU1Nx7ty5EamNoeYGfPDBB8jKysKLL76IEydOICEhAWlpaaitrZW6NLuSl5eH5cuX4/Dhw9i9eze6urowb9486HQ6qUuzW19++SX+/ve/D3nBWBq6pqYmzJw5Ey4uLvj8889x+vRp/PGPf4Svr6/UpdmVV199Fa+//jr++te/4rvvvsOrr76KdevW4S9/+YvUpdk8nU6HhIQEbNy4cdDj69atw4YNG7Bp0yYcOXIEnp6eSEtLQ0dHh+WLG/KKkDTAtGnTxOXLlxsf9/T0iKGhoeLatWslrMr+1dbWigDEvLw8qUuxSy0tLeLYsWPF3bt3i3PmzBGffvppqUuyK88995x46623Sl2G3bvrrrvEpUuXmuxbsGCB+NBDD0lUkX0CIGZnZxsfGwwGMSQkRPy///s/477m5mZRLpeL77//vsXr4Z2a69TZ2Ynjx48jNTXVuM/JyQmpqakoKCiQsDL7p9FoAAB+fn4SV2Kfli9fjrvuusvk/20aPjt27EBSUhLuu+8+BAUFITExEf/4xz+kLsvuzJgxA7m5uTh79iwA4Ouvv8bBgweRnp4ucWX2rbS0FNXV1Sb/fiiVSiQnJ4/Id6PDLGg53Orr69HT04Pg4GCT/cHBwThz5oxEVdk/g8GAFStWYObMmZg4caLU5didf/7znzhx4gS+/PJLqUuxWyUlJXj99deRlZWFX/7yl/jyyy/xs5/9DK6urli8eLHU5dmNX/ziF9BqtYiLi4NMJkNPTw9+97vf4aGHHpK6NLtWXV0NAIN+N/YfsySGGrIpy5cvx6lTp3Dw4EGpS7E75eXlePrpp7F79264ublJXY7dMhgMSEpKwu9//3sAQGJiIk6dOoVNmzYx1Ayjf/3rX9i2bRu2b9+OCRMmQK1WY8WKFQgNDeXnbMfY/HSdAgICIJPJUFNTY7K/pqYGISEhElVl35588kl89tln2LdvH0aNGiV1OXbn+PHjqK2txc033wxnZ2c4OzsjLy8PGzZsgLOzM3p6eqQu0S6oVCqMHz/eZF98fDzKysokqsg+rVq1Cr/4xS/wwAMP4KabbsKPfvQjrFy5EmvXrpW6NLvW//0n1XcjQ811cnV1xZQpU5Cbm2vcZzAYkJubi+nTp0tYmf0RRRFPPvkksrOzsXfvXkRHR0tdkl2aO3cuvvnmG6jVauOWlJSEhx56CGq1GjKZTOoS7cLMmTMHTElw9uxZREZGSlSRfWpra4OTk+lXnEwmg8FgkKgixxAdHY2QkBCT70atVosjR46MyHcjm59uQFZWFhYvXoykpCRMmzYN69evh06nw5IlS6Quza4sX74c27dvx7///W94e3sb22WVSiXc3d0lrs5+eHt7D+in5OnpCX9/f/ZfGkYrV67EjBkz8Pvf/x73338/jh49ijfeeANvvPGG1KXZlYyMDPzud79DREQEJkyYgK+++gqvvfYali5dKnVpNq+1tRVFRUXGx6WlpVCr1fDz80NERARWrFiBl19+GWPHjkV0dDReeOEFhIaGIjMz0/LFWXx8lZ37y1/+IkZERIiurq7itGnTxMOHD0tdkt0BMOj21ltvSV2a3eOQbsv49NNPxYkTJ4pyuVyMi4sT33jjDalLsjtarVZ8+umnxYiICNHNzU0cPXq0+Pzzz4t6vV7q0mzevn37Bv03efHixaIo9g7rfuGFF8Tg4GBRLpeLc+fOFQsLC0ekNkEUOb0iERER2T72qSEiIiK7wFBDREREdoGhhoiIiOwCQw0RERHZBYYaIiIisgsMNURERGQXGGqIiIjILjDUEBERkV1gqCEiIiK7wFBDREREdoGhhoiIiOwCQw0RERHZhf8PVA/fDycDV2IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "log_history = trainer.state.log_history\n",
    "losses = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
