## 手撕Transformer

文档：

1. Transformer模型架构介绍
2. 掩码(Masked)层

代码：

1. 注意力Attention原理和实现
2. token 和 position Embedding
3. LayerNorm
4. 残差连接FFN
5. Encoder-Decoder实现

## 手撕BERT

文档：

1. 图解BERT
2. 分词预处理
3. WordPiece讲解

## 手撕GPT

文档：

1. 图解GPT
2. BPE讲解
3. FlashAttention
4. flash attention讲解

代码：

1. BPE（Byte-Pair Encoding）分词算法
2. GPT模型
3. 大模型简单推理

## 手撕LLAMA

文档：

1. LLAMA大致讲解

2. RMS归一化

3. SwiGLU激活函数

4. 旋转式位置编码
5. 多查询注意力+分组查询注意力
6. KV-Cache推理加速

文档：

1. BPE（Byte-Pair Encoding）分词算法-封装为类的较完整版
2. llama模型实现
3. 简单trainer

## 手撕Alpaca

计算全量微调占显存

Stanford Alpaca: An Instruction-following LLaMA Model，代码： https://github.com/tatsu-lab/stanford_alpaca，运行教程：https://zhuanlan.zhihu.com/p/618321077

## 手撕LORA

LoRA/QLoRA论文解析

### LLM推理优化技术

VLLM

https://github.com/liguodongiot/llm-action/
