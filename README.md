## 手撕Transformer

文档：

1. Transformer模型架构介绍
2. 掩码(Masked)层

代码：

- 注意力Attention原理和实现
- token 和 position Embedding
- LayerNorm
- 残差连接FFN
- Encoder-Decoder实现

## 手撕BERT

文档：

1. 图解BERT
2. 分词预处理
3. WordPiece讲解

## 手撕GPT

文档：

1. 图解GPT
2. BPE讲解
3. FlashAttention

代码：

1. BPE（Byte-Pair Encoding）分词算法
2. GPT模型
3. 大模型简单推理
4. flash attention讲解

## 手撕LLAMA

文档：

LLAMA大致讲解

RMS归一化

SwiGLU激活函数

旋转式位置编码


## 手撕Alpaca

计算全量微调占显存

Stanford Alpaca: An Instruction-following LLaMA Model，代码： https://github.com/tatsu-lab/stanford_alpaca，运行教程：https://zhuanlan.zhihu.com/p/618321077

## 手撕LORA

LoRA/QLoRA论文解析

### LLM推理优化技术

VLLM

https://github.com/liguodongiot/llm-action/
