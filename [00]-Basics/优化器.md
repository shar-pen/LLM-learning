# 优化器讲解

## SGD随机梯度下降

随机梯度下降是对每个训练样本就更新一次网络参数，这样使得网络更新参数速度很快，但是问题就是由于训练数据多样，容易朝偏离网络最优点方向训练，网络训练不稳定。

```Python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for example in data:
        params_grad= evaluate_gradient(loss_function, example, params)
        params =params - learning_rate * params_grad

```



## 动量法

思想：让参数的更新具有惯性，加快收敛能帮助参数在正确的方向上加速前进。

比如小球在下坡山坡上，梯度方向是一直向下的，自然参数更新幅度也就是一直累加的，也就变得越来越大；而当遇到山沟，越过山沟此时就在另一边山坡，这个时候梯度方向是跟之前相反的，此时由于之前梯度大小的累加，在两个山坡间的变化就会被互相抵消掉，也就不会一直在两个山坡振荡，容易朝山沟向下走，也就是减少摇摆了。

$$
\begin{array}{l}\mathbf{v}_{t} \leftarrow \beta \mathbf{v}_{t-1}+\mathbf{g}_{t}, \\ \mathbf{x}_{t} \leftarrow \mathbf{x}_{t-1}-\eta_{t} \mathbf{v}_{t} .\end{array}
$$

$\mathbf{g}_{t}$为梯度，$\beta$为动量参数，通常被设置为0.9



## Adagrad

**Adagrad 思想：对于不同参数，设置不同的学习率。**

Adagrad优化算法被称为自适应学习率优化算法，之前讲的随机梯度下降法，对所有的参数，都是使用相同的、固定的学习率进行优化的，但是不同的参数的梯度差异可能很大，使用相同的学习率，效果不会很好。

打个比方：

假设我们用一批数据训练网络，这个数据中只有少部分数据含有某个特征，另一个特征几乎全部数据都具有，当这些数据通过训练时，对于不同特征我们假设对应于不同的神经元权重，对于都含有的特征，这些神经元对应参数更新很快，但是对于那些只有少部分数据含有的特征，对应神经元权重获得更新机会就少，但是由于学习率一样，这样可能导致神经网络训练的不充分。

$$
\begin{aligned} \mathbf{s}_{t} & =\mathbf{s}_{t-1}+\mathbf{g}_{t}^{2} \\ \mathbf{w}_{t} & =\mathbf{w}_{t-1}-\frac{\eta}{\sqrt{\mathbf{s}_{t}+\epsilon}} \cdot \mathbf{g}_{t}\end{aligned}
$$

$\eta$是学习率，$\epsilon$是一个为维持数值稳定性而添加的常数，用来确保不会除以0。

这样，不同的参数由于梯度不同，他们对应的$s_{t}$大小也就不同，所以学习率也就不同，这也就实现了自适应的学习率。

总结： **Adagrad** 的核心想法就是，**如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点**，防止震荡，**而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点**，使得其能够更快地更新，这就是Adagrad算法加快深层神经网络的训练速度的核心。



## RMSProp

adagrad有个问题，就是其分母随着训练数增加，也会跟着增加，这样会导致学习速率越来越小，最终变的无限小，从而无法有效更新参数。

RMSProp：Root Mean Square Propagation 均方根传播

RMSProp 是在 adagrad 的基础上，进一步在学习率的方向上优化，修改了AdaGrad的梯度积累为指数加权的移动平均

$$
\begin{array}{l}\mathbf{s}_{t} \leftarrow \gamma \mathbf{s}_{t-1}+(1-\gamma) \mathbf{g}_{t}^{2} \\ \mathbf{x}_{t} \leftarrow \mathbf{x}_{t-1}-\frac{\eta}{\sqrt{\mathbf{s}_{t}+\epsilon}} \odot \mathbf{g}_{t} .\end{array}
$$

$\gamma$一般为0.9



## Adam

Adam算法的关键组成部分之一是：它使用**指数加权移动平均值**来估算梯度的动量和二次矩，即它使用状态变量

$$
\begin{array}{l}\mathbf{v}_{t} \leftarrow \beta_{1} \mathbf{v}_{t-1}+\left(1-\beta_{1}\right) \mathbf{g}_{t}, \\ \mathbf{s}_{t} \leftarrow \beta_{2} \mathbf{s}_{t-1}+\left(1-\beta_{2}\right) \mathbf{g}_{t}^{2} .\end{array}
$$

这里$\beta_{1}$和$\beta_{2}$是非负加权参数。 常将它们设置为$\beta_{1}=0.9$和$\beta_{2}=0.999$。 也就是说，方差估计的移动远远慢于动量估计的移动。注意，如果我们初始化$\mathbf{v}_{0}=\mathbf{s}_{0}=0$，那么初始值很小接近为0，因此重新计算一个偏差来校正：

$$
\hat{\mathbf{v}}_{t}=\frac{\mathbf{v}_{t}}{1-\beta_{1}^{t}}\\\hat{\mathbf{s}}_{t}=\frac{\mathbf{s}_{t}}{1-\beta_{2}^{t}}
$$

接着对用非常类似于RMSProp算法的方式重新缩放梯度

$$
\mathbf{g}_{t}^{\prime}=\frac{\eta \hat{\mathbf{v}}_{t}}{\sqrt{\hat{\mathbf{s}}_{t}}+\epsilon}
$$

与RMSProp不同，我们的更新使用动量$\hat{\mathbf{v}}_{t}$而不是梯度本身。 此外，由于使用$\frac{1}{\sqrt{\hat{\mathbf{s}}_{t}}+\epsilon}$而不是$\frac{1}{\sqrt{\hat{\mathbf{s}}_{t}+\epsilon}}$进行缩放，两者会略有差异。 前者在实践中效果略好一些，因此与RMSProp算法有所区分。

最后，简单更新参数：

$$
\mathbf{x}_{t} \leftarrow \mathbf{x}_{t-1}-\mathbf{g}_{t}^{\prime}
$$

回顾Adam算法，它的设计灵感很清楚： 首先，动量和规模在状态变量中清晰可见， 它们相当独特的定义使我们移除偏项（这可以通过稍微不同的初始化和更新条件来修正）。 其次，RMSProp算法中两项的组合都非常简单。


## 参考
[深度学习各类优化器详解（动量、NAG、adam、Adagrad、adadelta、RMSprop、adaMax、Nadam、AMSGrad）_动量优化器-CSDN博客](https://blog.csdn.net/qq_42109740/article/details/105401197)

[优化器（凸性、梯度、动量、Adagrad、RMSProp及Adam优化）_动量优化器-CSDN博客](https://blog.csdn.net/zhu_xian_gang/article/details/134069387?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Ctr-1-134069387-blog-105401197.235^v43^pc_blog_bottom_relevance_base5&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Ctr-1-134069387-blog-105401197.235^v43^pc_blog_bottom_relevance_base5&utm_relevant_index=1)

[11.8. RMSProp算法 — 动手学深度学习 2.0.0 documentation](https://zh.d2l.ai/chapter_optimization/rmsprop.html)

https://www.bilibili.com/video/BV1YF411n7Dr/?spm_id_from=333.337.search-card.all.click&vd_source=df8edb354538b32afc637d2d9df59b04