WordPiece是Google为[预训练](https://so.csdn.net/so/search?q=预训练&spm=1001.2101.3001.7020)BERT开发的分词算法，它被应用于基于BERT的Transformer模型，如DistilBERT、MobileBERT、Funnel Transformers和MPNET等。它与BPE在训练方法上相似，但实际的分词过程有所不同。

💡 这部分深入讲解WordPiece，甚至会展示完整的实现。如果你只想了解分词算法的概要，可以跳到最后。

# 训练算法

⚠️ Google并未公开其WordPiece训练算法的实现，以下内容基于已发表文献的推测，可能不完全准确。

与BPE类似，WordPiece从包含模型特殊符号和初始字母表的小词汇表开始。由于**它通过添加前缀（如BERT中的`##`）来识别子词**（子词确实与正常词含义通常不一样），每个单词最初都是通过在单词内部添加该前缀来分割的。例如，"word"会被这样分割：

```
w ##o ##r ##d
```

因此，初始字母表包含单词开头的所有字符以及带有WordPiece前缀的单词内部字符。

然后，与BPE类似，WordPiece学习合并规则。主要区别在于合并对的选择方式。WordPiece不会选择最频繁的对，而是为每对计算分数，使用以下公式：
$$
score=(freq_{pair})/(freq_{1st-element}×freq_{2nd-element})
$$
score分数越高，代表这两个子词的出现频率不如他俩的组合，因此这两个子词组合起来效益越高。

通过将对的频率除以每个部分频率的乘积，算法优先合并那些部分在词汇表中频率较低的对。例如，即使`("un", "##able")`这对在词汇表中出现频率很高，WordPiece也不一定会将其合并，因为`"un"`和`"##able"`可能分别出现在许多其他单词中，频率较高。相比之下，像`("hu", "##gging")`这样的对可能会更快地合并（假设词汇表中"hugging"出现频繁），因为`"hu"`和`"##gging"`作为单独的词可能频率较低。

让我们再次看训练示例中的词汇表：

1. ("hug", 10)
2. ("pug", 5)
3. ("pun", 12)
4. ("bun", 4)
5. ("hugs", 5)

这里的分割结果会是：

1. ("h" "##u" "##g", 10)
2. ("p" "##u" "##g", 5)
3. ("p" "##u" "##n", 12)
4. ("b" "##u" "##n", 4)
5. ("h" "##u" "##gs", 5)

因此，初始词汇表将是`["b", "h", "p", "##g", "##n", "##s", "##u"]`（忽略特殊符号）。最频繁的对是`("##u", "##g")`（出现20次），但由于`"##u"`的频率很高，其分数不是最高的（为1/36）。所有包含`"##u"`的对实际上都有相同的分数（1/36），所以最好的分数给了没有`"##u"`的对`("##g", "##s")`，分数为1/20，第一个学习的合并是`("##g", "##s") -> ("##gs")`。

请注意，当我们合并时，会移除两个标记之间的`##`，所以我们添加`"##gs"`到词汇表中，并在语料库中的单词中应用合并：

Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)

到目前为止，所有可能的对`"##u"`都有相同的分数。假设在这种情况下，我们首先合并第一个对，即`("h", "##u") -> "hu"`。这样我们就到了：

Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)

接下来最好的分数由`("hu", "##g")`和`("hu", "##gs")`共享（分数为1/15，而其他所有对的分数为1/21），所以我们首先合并分数最高的对：

Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)

然后我们继续这个过程，直到达到所需的词汇表大小。

# 分词算法

**WordPiece和BPE在分词方面的不同在于，WordPiece只保存最终的词汇表，而不保存学习到的合并规则。从要分词的单词开始，WordPiece会找到词汇表中以该单词开头的最长子词，然后在该位置进行分割。**例如，如果我们使用上述示例中的词汇表，对于单词`"hugs"`，从开头开始的最长在词汇表中的子词是`"hug"`，所以我们在此处分割，得到`["hug", "##s"]`。然后我们继续处理`"##s"`，它也在词汇表中，所以`"hugs"`的分词结果是`["hug", "##s"]`。

BPE则会应用学到的合并规则，将`"hugs"`分词为`["hu", "##gs"]`，因此编码方式不同。

再举个例子，我们来看看单词`"bugs"`的分词过程。从单词开头开始，最长在词汇表中的子词是`"b"`，所以我们在此处分割，得到`["b", "##ugs"]`。然后在`"##ugs"`中，最长的以开头的子词是`"##u"`，所以我们在此分割，得到`["b", "##u", "##gs"]`。最后，`"##gs"`在词汇表中，所以这个列表就是`"bugs"`的分词结果。

当分词到达无法在词汇表中找到子词的阶段时，整个单词将被标记为未知——例如，单词`"mug"`会被分词为`["[UNK]"]`，就像`"bum"`（尽管我们可以从`"b"`和`"##u"`开始，但`"##m"`不在词汇表中，最终分词结果将是`["[UNK]"]`，而不是`["b", "##u", "[UNK]"]`）。这与BPE不同，它只会将单个不在词汇表中的字符标记为未知。

📝 **现在轮到你了！** `"pugs"`这个单词会被如何分词？



解释一下 **WordPiece** 和 **BPE（Byte Pair Encoding）** 在分词方式上的区别：

------

### **WordPiece：基于已知词片的模式匹配**

- **基于固定词汇表**：
   WordPiece 分词依赖于一个预先构建好的词汇表（vocabulary），这个词汇表是在训练时根据语料库生成的，包含了所有的子词（subword）。分词时，它会根据词汇表中的已知词片对输入文本进行匹配。
- **最长匹配优先（贪心算法）**：
   WordPiece 会从左到右扫描文本，并尝试找到词汇表中与当前文本段匹配的最长子词。
  - 例如，分词 unbelievable
    - 如果词汇表中包含 `["un", "##believable", "##able", "##lie", "##vable"]`。
    - 输出结果为：`[un, ##believable]`。
  - 如果某个单词完全不在词汇表中（OOV），则会进一步拆分为更小的子词。
- **效率高**：
   因为分词过程完全依赖于词汇表，直接进行匹配，不需要动态生成规则。

------

### **BPE：逐步合并符号对**

- **动态合并**：
   BPE 分词从字符级别开始，将最常见的符号对（例如两个字符或子词）逐步合并，最终形成新的子词。
  - BPE 的核心在于合并频率最高的符号对，直到达到预定的词汇表大小。
- **词汇表生成过程**：
   在训练时，BPE 会分析语料库，逐步合并出现频率最高的符号对，构建子词的词汇表。
  - 例如，在合并 unbelievable
    - 第一步：合并频率最高的 `u` 和 `n`，得到 `un`和`believable`。
    - 经过多次合并后：`unbelievable`。
- **分词过程**：
   在实际分词时，BPE 根据训练时学到的合并规则，对输入文本重复执行合并操作，直到无法再合并或所有子词都在词汇表中。

------

### **对比总结**：

| 特性               | **WordPiece**                                          | **BPE**                                                  |
| ------------------ | ------------------------------------------------------ | -------------------------------------------------------- |
| **词汇表构建方式** | 基于语料库的模式匹配，生成能最大化语料似然的子词单元。 | 按符号对的出现频率逐步合并，生成固定大小的词汇表。       |
| **分词方式**       | 根据已知词汇表中的子词，采用最长匹配优先策略进行分词。 | 根据已学到的合并规则动态合并，逐步生成分词结果。         |
| **处理生僻词**     | 将词拆分为已知的小子词，保证能被词汇表覆盖。           | 按规则将生僻词逐步拆分为字符或较小单元，直到词汇表匹配。 |
| **效率**           | 依赖词汇表，分词快速且确定性强。                       | 分词过程动态，稍微慢一些，因为需要重复合并。             |
| **灵活性**         | 依赖固定词汇表，灵活性略差。                           | 动态合并规则，灵活性更强。                               |

------

### **重点区别**：

1. **分词策略**：
   - WordPiece 分词是基于固定词汇表的子词模式匹配，更像是直接查表，效率高。
   - BPE 则是基于合并规则逐步生成子词，分词过程更灵活，但需要动态处理。
2. **生成规则**：
   - WordPiece 的词汇表构建是为了最大化语料库的似然，结合了统计信息和语言模型目标。
   - BPE 的词汇表构建完全基于符号对的合并频率，更简单直观。

------

### **总结**：

- 如果任务对速度和确定性要求较高，WordPiece 是更好的选择，因为它直接基于已知词汇表进行匹配。
- 如果需要动态适应新的场景或处理不同语言，BPE 由于其灵活性会更适合。

# 参考：

WordPiece tokenization: https://huggingface.co/learn/nlp-course/en/chapter6/6, 翻译版https://blog.csdn.net/aa12367/article/details/138300139