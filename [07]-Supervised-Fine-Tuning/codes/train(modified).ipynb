{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "改进Alpaca源码:\n",
    "\n",
    "- 原始Alpaca是自己读取并预处理数据，这里我改为dataset读取并用map函数批量预处理。\n",
    "- 原始Alpaca训练7B模型需要四张A100卡，不算数据单独模型训练也需要100GB以上的显存，这里我改为bf16训练，可用一张A800(80G)训练。更改的地方包括 \n",
    "    1. 模型加载，加载时设置torch_dtype\n",
    "    2. 训练参数，增加bf16=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = '../DataCollection/officials/Llama-2-7b'\n",
    "# model_name_or_path = '../DataCollection/officials/Qwen2.5-1.5b-Instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', \n",
    "                       data_dir='/data02/hyzhang10/pengxia2/tws/data', \n",
    "                       data_files={\n",
    "                           'train': 'alpaca_data_100.json', \n",
    "                        #    'test': 'alpaca_data_100.json'\n",
    "                           }\n",
    "                       )\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset['train']))\n",
    "pprint(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "\n",
    "def preprocess_func(example):\n",
    "    source = prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "    target = f\"{example['output']}{tokenizer.eos_token}\"\n",
    "    full_example = source + target\n",
    "    full_example_tokenzied = tokenizer(full_example, return_tensors=\"pt\",padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True)\n",
    "    input_ids = full_example_tokenzied['input_ids'][0]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    source_tokenzied = tokenizer(source, return_tensors=\"pt\",padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True)\n",
    "    labels[:len(source_tokenzied['input_ids'][0])] = IGNORE_INDEX\n",
    "    return dict(\n",
    "        input_ids=input_ids, \n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "\n",
    "# preprocess_func(dataset['train'][0])\n",
    "train_ds = dataset['train'].map(preprocess_func, remove_columns=list(dataset['train'].features.keys()))\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(input_ids) for input_ids in train_ds['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(train_ds[0], width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DataCollatorForSeq2Seq(tokenizer)\n",
    "# DataCollator的输入是list[dict[str, tensor]]\n",
    "ret = [train_ds[index] for index in range(2,4)]\n",
    "ret = dc(ret)\n",
    "print([len(input_ids) for input_ids in ret['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_ds\n",
    "eval_dataset=None\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./output',\n",
    "                                  num_train_epochs=3,\n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=8,\n",
    "                                  gradient_accumulation_steps=8,\n",
    "                                  evaluation_strategy='no',\n",
    "                                  save_strategy='steps',\n",
    "                                  save_steps=2000,\n",
    "                                  save_total_limit=1,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.,\n",
    "                                  warmup_ratio=0.03,\n",
    "                                  lr_scheduler_type='cosine',\n",
    "                                  logging_steps=1,\n",
    "                                  report_to=[],\n",
    "                                  bf16=True\n",
    "                                  )\n",
    "trainer = Trainer(model=model, \n",
    "                  tokenizer=tokenizer, \n",
    "                  args=training_args, \n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=eval_dataset,\n",
    "                  data_collator=data_collator,\n",
    "                  )\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "losses = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
