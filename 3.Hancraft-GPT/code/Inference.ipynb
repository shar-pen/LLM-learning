{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation/Inference\n",
    "\n",
    "https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This blog post gives a brief overview of different decoding strategies. \n",
    "All of the following functionalities can be used for auto-regressive language generation, https://jalammar.github.io/illustrated-gpt2/. \n",
    "In short, auto-regressive language generation is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions. \n",
    "\n",
    "We will give a tour of the currently most prominent decoding methods, mainly \n",
    "- Greedy search\n",
    "- Beam search\n",
    "- Top-K sampling\n",
    "- Top-p sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "DEVICE = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# local_cache_dir is for mannually downloading model params to local env\n",
    "local_cache_dir = \"../../DataCollection/officials/gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(local_cache_dir, output_hidden_states=True).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a naive implementation of GPT2 generating text based on Transformers package. \n",
    "Its HF repostory is at https://huggingface.co/openai-community/gpt2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " What is star war?\n",
      "\n",
      "Star wars are the most common form of warfare in the world. The most common form of warfare is the war of attrition. The most common form of warfare is the war of attrition.\n",
      "\n",
      "Star wars are the most common form of warfare in the world. The most common form of warfare is the war of attrition. The most common form of warfare is the war of attrition.\n",
      "\n",
      "Star wars are the most common form of warfare in the world. The most common form of warfare is\n"
     ]
    }
   ],
   "source": [
    "# Encode initial input\n",
    "input_text = \"What is star war?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(DEVICE)  # Shape: [1, 4]\n",
    "\n",
    "# Set the number of tokens to generate\n",
    "num_tokens_to_generate = 100\n",
    "\n",
    "# Iteratively generate tokens\n",
    "# for _ in tqdm(range(num_tokens_to_generate), mininterval=1):\n",
    "for _ in range(num_tokens_to_generate):\n",
    "\n",
    "    # Get model output logits\n",
    "    outputs = model(input_ids)  # Shape: [1, current_length, 50257] or [batch_size, token length, vocab size]\n",
    "    logits = outputs.logits\n",
    "\n",
    "    '''\n",
    "    Predict the next token based on the last position\n",
    "    i.e., the i-th position logits is for predicting the i+1-th token\n",
    "    In this case, we want to predict the next token based on previous tokens, so we use the logits of the final token.\n",
    "    If you see the source code of forward function, you can notice the shifting of labels and logits for aligning.\n",
    "    '''\n",
    "    next_token_logits = logits[:, -1, :]  # Shape: [1, 50257], corresponding to each vocab\n",
    "\n",
    "    '''\n",
    "    Greedy decoding: select the token with the highest probability\n",
    "    Supposily you can try top-k and beam search\n",
    "    '''\n",
    "    greedy_token_id = torch.argmax(next_token_logits, dim=-1)  # Shape: [1]\n",
    "\n",
    "    # Append the predicted token to the input_ids\n",
    "    input_ids = torch.cat([input_ids, greedy_token_id.unsqueeze(-1)], dim=-1).to(DEVICE)  # Shape: [1, current_length + 1]\n",
    "\n",
    "    # print(tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True))\n",
    "\n",
    "# Decode the entire sequence of tokens\n",
    "generated_text = tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 104, 50257])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# official functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:7'),\n",
      " 'input_ids': tensor([[   40,  2883,  6155,   351,   616, 13779,  3290]], device='cuda:7')}\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(DEVICE)\n",
    "\n",
    "pprint(model_inputs, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Greedy Search\n",
    "\n",
    "\n",
    "Selects the word with the highest probability as its next word at each timestep. The `generate` function use this strategy as default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# generate 40 new tokens\n",
    "# the output of generate is a `GenerateDecoderOnlyOutput` object, we only need the first attribute.\n",
    "greedy_output = model.generate(**model_inputs, \n",
    "    max_new_tokens=40, \n",
    "    # max_length=50, \n",
    "    )\n",
    "\n",
    "token_ids = torch.squeeze(greedy_output[0])\n",
    "print(tokenizer.decode(token_ids, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search. \n",
    "\n",
    "The major drawback of greedy search though is that it misses high probability words hidden behind a low probability word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Beam search\n",
    "\n",
    "Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. So eventually we still get one sequence. \n",
    "\n",
    "Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "token_ids = torch.squeeze(beam_output[0])\n",
    "print(tokenizer.decode(token_ids, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the result is arguably more fluent, the output still includes repetitions of the same word sequences.\n",
    "A simple remedy is to introduce n-grams (a.k.a word sequences of words) penalties. \n",
    "\n",
    "The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output (Beam Search)(n-grams penalty)]: \n",
      "I enjoy walking with my cute dog, but I don't think I'll ever be able to walk with her again.\"\n",
      "\n",
      "\"You're right,\" she said. \"I'm going to have to get used to it. I\n"
     ]
    }
   ],
   "source": [
    "# introduce n-grams (a.k.a word sequences of n words) penalties\n",
    "# by default, this penalty will set the possibiliy to 0\n",
    "# The repetition_penalty parameter can be set to discourage the model from generating repeated n-grams. A value greater than 1.0 penalizes repetition. \n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    repetition_penalty=1.5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"[Output (Beam Search)(n-grams penalty)]: \")\n",
    "token_ids = torch.squeeze(beam_output[0])\n",
    "print(tokenizer.decode(token_ids, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple outcomes of a single generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `num_return_sequences`, you can get multiple beams, applicable in both beam search and sampling methods. \n",
    "Notes that by default, generate will use greedy search, so you will get the same sequence no matter the num_return_sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "====================\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "====================\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea to\n",
      "====================\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time to take a\n",
      "====================\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "token_ids = torch.squeeze(beam_output[0])\n",
    "for j in range(token_ids.shape[0]):\n",
    "    print(tokenizer.decode(token_ids[j], skip_special_tokens=True))\n",
    "    print(20*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As argued in Ari Holtzman et al. (2019), high quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. \n",
    "\n",
    "1. Token Probabilities: After the model processes the input text, it predicts a probability distribution over the possible next tokens. \n",
    "2. Filtering to Top-k: Instead of considering all possible tokens, top-k sampling narrows down the choices to the k tokens with the highest probabilities. This \"pruning\" reduces the potential output space, focusing on the most probable next tokens while ignoring less likely ones. \n",
    "3. Random Sampling: From the top-k tokens, one token is sampled randomly according to their probabilities, rather than always choosing the highest probability token. This introduces variety into the generated text, leading to more diverse outputs.\n",
    "4. Controlling Output Diversity: By adjusting the value of k, High k (e.g., 50 or 100) allows more options, increasing diversity and potentially creativity, at the risk of less coherence. Low k (e.g., 5 or 10) limits options, usually making the text more deterministic and focused but sometimes too repetitive or safe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog Molly. We share a bit and we're sure we'll be spending a lot less.\n",
      "\n",
      "I always have the sneaking suspicion that sometimes pets get a bit petty, but it turns out\n"
     ]
    }
   ],
   "source": [
    "topk_output = model.generate(**model_inputs, \n",
    "    max_new_tokens=40,\n",
    "    do_sample=True, \n",
    "    top_k=50\n",
    "    )\n",
    "\n",
    "token_ids = torch.squeeze(topk_output[0])\n",
    "print(tokenizer.decode(token_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One concern though with Top-K sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution. This can be problematic as some words might be sampled from a very sharp distribution (distribution focused on few words), whereas others from a much more flat distribution. \n",
    "Thus, limiting the sample pool to a fixed size K could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. \n",
    "\n",
    "It only differs from top-k in terms of filtering. Instead of selecting the top-k tokens with the highest individual probabilities, top-p sampling considers the smallest set of tokens whose cumulative probability is at least p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog and she is so much fun. She always is happy to meet my furry friends. She has a very friendly, warm, friendly demeanor and she likes playing with my pet! I am a huge fan\n"
     ]
    }
   ],
   "source": [
    "topp_output = model.generate(**model_inputs, \n",
    "    max_new_tokens=40,\n",
    "    do_sample=True, \n",
    "    top_p=0.92\n",
    "    )\n",
    "\n",
    "token_ids = torch.squeeze(topp_output[0])\n",
    "print(tokenizer.decode(token_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top-p and top-K sampling seem to produce more fluent text than traditional greedy - and beam search on open-ended language generation. Recently, there has been more evidence though that the apparent flaws of greedy and beam search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "DEVICE = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# local_cache_dir is for mannually downloading model params to local env\n",
    "local_cache_dir = \"../../DataCollection/officials/gpt2\"\n",
    "\n",
    "pipe = pipeline(task='text-generation', model=local_cache_dir, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pipe.tokenizer.pad_token_id:\n",
    "    pipe.tokenizer.pad_token_id = pipe.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time when no one was interested in having a female character, the character was developed as a romantic drama. And she is a character you need to be a part of. The reason why women want roles as characters is because it's something that we are not\n",
      "====================\n",
      "Once upon a time, even the very best minds in the world knew that they were no longer a part of the great machine we had built for ourselves. The people began to understand that a whole new era could be started.\n",
      "\n",
      "As a result of this knowledge,\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "result = pipe(\"Once upon a time\", \n",
    "            max_new_tokens=50, \n",
    "            # top_k=50, \n",
    "            top_p=0.92,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=2,\n",
    "            )\n",
    "for item in result:\n",
    "    print(item['generated_text'])\n",
    "    print(20*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: The future of technology is\n",
      "The future of technology isThe future of technology is changing more quickly than ever. The ability to connect to your computer and get access to and manipulate your information is gaining momentum, and companies are starting to realize that it is extremely valuable.\n",
      "====================\n",
      "Prompt 2: Once upon a time in a distant land\n",
      "Once upon a time in a distant land when I was a boy, my father asked me why I'd joined the British Army. I explained that it was for the military's own bad. I remember being in a room full of young men with red\n",
      "====================\n",
      "Prompt 3: Artificial intelligence has changed\n",
      "Artificial intelligence has changedIt has become too big, too high and too expensive for our everyday lives, with the main goal of giving us all the tools we need for achieving our goals or making sure we're doing something good. For\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"The future of technology is\", \"Once upon a time in a distant land\", \"Artificial intelligence has changed\"]\n",
    "results = pipe(prompts, max_length=50, batch_size=8)\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    print(f\"Prompt {idx + 1}: {prompts[idx]}\")\n",
    "    print(result[0]['generated_text'])\n",
    "    print(20*'=')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
