Byte-Pair Encoding（BPE）最初是作为一种文本压缩算法开发的，由Sennrich等人于2015年引入到NLP领域并很快得到推广，可参考[Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909) 。后来被[OpenAI](https://so.csdn.net/so/search?q=OpenAI&spm=1001.2101.3001.7020)用于GPT模型的预训练时的tokenization。许多Transformer模型，如GPT、GPT-2、RoBERTa、BART和DeBERTa，都使用了BPE。

## 训练算法

BPE训练首先计算语料库中（在标准化和预tokenization步骤完成后）使用的独特单词集，然后通过使用书写这些单词的所有符号来构建词汇表。以一个简单的例子来说，假设我们的语料库包含以下五个单词：

```
"hug", "pug", "pun", "bun", "hugs"
```

初始词汇表将是`["b", "g", "h", "n", "p", "s", "u"]`。**在实际情况下，初始词汇表将至少包含所有ASCII字符，可能还包括一些Unicode字符。如果正在tokenizing的示例中使用了训练语料库中未包含的字符，该字符将被转换为未知令牌。这就是为什么许多NLP模型在分析带有表情符号的内容时表现不佳的一个原因**。

GPT-2和RoBERTa的tokenizer（它们非常相似）有一个巧妙的处理方式：它们不将单词视为由Unicode字符组成的，而是由字节组成。这样，初始词汇表的大小较小（256），但你想到的任何字符都会被包含在内，不会被转换为未知令牌。这种技巧称为*字节级BPE*。

在获取了基础词汇表后，我们通过学习**合并**（merges）来添加新的令牌，直到达到所需的词汇表大小。合并是将现有词汇表中的两个元素合并成一个新的规则。**起初，这些合并会创建包含两个字符的令牌，随着训练的进行，会生成更长的子词**。

在tokenizer训练的任何阶段，BPE算法都会搜索现有词汇表中最频繁的两个令牌对（这里的“对”是指单词中的连续两个令牌）。最频繁的这对将被合并，然后我们继续进行下一轮。

回到之前的例子，假设这些单词的频率如下：

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

这意味着“hug”在语料库中出现了10次，“pug”5次，“pun”12次，“bun”4次，“hugs”5次。我们从将每个单词分解成字符（构成我们的初始词汇表）开始，以便将每个单词视为一个令牌列表：

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

然后我们查看对。对`("h", "u")`在“hug”和“hugs”中出现，总共在语料库中出现了15次。但这不是最频繁的对：最频繁的是`("u", "g")`，它在“hug”、“pug”和“hugs”中出现，总共在词汇表中出现了20次。

因此，**tokenizer学习的第一个合并规则是`("u", "g") -> "ug"`，这意味着“ug”将被添加到词汇表中，并在语料库中的所有单词中合并**。经过这一阶段，词汇表和语料库看起来像这样：

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

现在我们有一些产生超过两个字符的令牌的对：例如对`("h", "ug")`（在语料库中出现15次）。现阶段最频繁的对是`("u", "n")`，它在语料库中出现了16次，所以我们学习到的第二个合并规则是`("u", "n") -> "un"`。将这个规则添加到词汇表中，并合并所有现有的出现，我们得到：

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

现在最频繁的对是`("h", "ug")`，所以我们学习到的合并规则是`("h", "ug") -> "hug"`，从而得到了我们的第一个三个字符的令牌。合并后，语料库看起来像这样：

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

我们继续这个过程，直到达到所需的词汇表大小。

## Tokenization algorithm

令牌化过程紧密地遵循训练过程，新输入通过以下步骤进行令牌化：

1. 标准化
2. 预令牌化
3. 将单词分割成单个字符
4. 按照学习到的合并规则对这些分割进行处理

让我们以训练过程中使用的三个合并规则为例：

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

单词`"bug"`会被令牌化为`["b", "ug"]`。然而，`"mug"`会被令牌化为`["[UNK]", "ug"]`，因为字母`"m"`不在基础词汇表中。同样，单词`"thug"`会被令牌化为`["[UNK]", "hug"]`：字母`"t"`不在基础词汇表中，应用合并规则首先将`"u"`和`"g"`合并，然后将`"h"`和`"ug"`合并。